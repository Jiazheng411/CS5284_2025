{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture : Graph SVM\n",
    "\n",
    "## Lab 01 : Standard/Linear SVM -- Exercise\n",
    "\n",
    "### Xavier Bresson, Guoji Fu \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Google Colaboratory\n",
    "import sys, os\n",
    "if 'google.colab' in sys.modules:\n",
    "    # mount google drive\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')\n",
    "    path_to_file = '/content/gdrive/My Drive/CS5284_2024_codes/codes/04_Graph_SVM'\n",
    "    print(path_to_file)\n",
    "    # change current path to the folder containing \"path_to_file\"\n",
    "    os.chdir(path_to_file)\n",
    "    !pwd\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "%matplotlib inline\n",
    "#%matplotlib notebook \n",
    "from matplotlib import pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'figure.max_open_warning': 0})\n",
    "from IPython.display import display, clear_output\n",
    "import time\n",
    "import sys; sys.path.insert(0, 'lib/')\n",
    "from lib.utils import compute_purity\n",
    "import warnings; warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linearly separable data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "mat = scipy.io.loadmat('datasets/data_linearSVM.mat')\n",
    "Xtrain = mat['Xtrain']\n",
    "Cgt_train = mat['Cgt_train'] - 1; Cgt_train = Cgt_train.squeeze()\n",
    "l_train = mat['l'].squeeze()\n",
    "n = Xtrain.shape[0]\n",
    "d = Xtrain.shape[1]\n",
    "nc = len(np.unique(Cgt_train))\n",
    "print(n,d,nc)\n",
    "Xtest = mat['Xtest']\n",
    "Cgt_test = mat['Cgt_test'] - 1; Cgt_test = Cgt_test.squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "plt.figure(figsize=(8,4))\n",
    "p1 = plt.subplot(121)\n",
    "size_vertex_plot = 100\n",
    "plt.scatter(Xtrain[:,0], Xtrain[:,1], s=size_vertex_plot*np.ones(n), c=Cgt_train, color=pyplot.jet())\n",
    "plt.title('Training Data')\n",
    "p2 = plt.subplot(122)\n",
    "size_vertex_plot = 100\n",
    "plt.scatter(Xtest[:,0], Xtest[:,1], s=size_vertex_plot*np.ones(n), c=Cgt_test, color=pyplot.jet())\n",
    "plt.title('Test Data')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: Implement the linear SVM on linear separable data using the primal-dual iterative algorithm\n",
    "\n",
    "*Hint:* Follow Page 18-20, Lecture 4 Slides\n",
    "\n",
    "**Step 1:** Compute the Linear Kernel $Ker$ and $L, Q$ defined as\n",
    "- $Ker= XX^\\top$,\n",
    "- $L = \\text{diag}(l)$, \n",
    "- $Q = LKL$.\n",
    " \n",
    "You may use function `np.diag()`, the transpose operator `.T`, and the matrix-matrix multiplication operator `.dot()`.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute linear kernel, L, Q\n",
    "\n",
    "l = l_train\n",
    "\n",
    "############################################################################\n",
    "# Your code start\n",
    "############################################################################\n",
    "\n",
    "Ker = \n",
    "L = \n",
    "Q = \n",
    "\n",
    "############################################################################\n",
    "# Your code end\n",
    "############################################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2:** Initialize $\\alpha^{k=0} = \\beta^{k=0} = 0_n$.\n",
    "\n",
    "You may use function `np.zeros()` for initializing a zero vector.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization\n",
    "############################################################################\n",
    "# Your code start\n",
    "############################################################################\n",
    "\n",
    "alpha = \n",
    "beta = \n",
    "\n",
    "############################################################################\n",
    "# Your code end\n",
    "############################################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3:** Choose the time steps $\\tau_\\alpha, \\tau_\\beta$ such that $\\tau_\\alpha\\tau_\\beta \\leq \\frac{1}{\\|Q\\| \\cdot \\|L\\|}$.\n",
    "\n",
    "Some feasible choices can be $\\tau_\\alpha = \\frac{a}{\\|Q\\|}, \\tau_\\beta = \\frac{b}{\\|L\\|}$, where $ab \\leq 1$.\n",
    "\n",
    "You may use `np.linalg.norm()` to compute the norm of a matrix.\n",
    "\n",
    "Try to evaluate the performance of linear SVM with different choices of time steps.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time steps\n",
    "############################################################################\n",
    "# Your code start\n",
    "############################################################################\n",
    "\n",
    "tau_alpha = \n",
    "tau_beta = \n",
    "\n",
    "############################################################################\n",
    "# Your code end\n",
    "############################################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4:** Project alpha to $[0, +\\infty]$ during the update of alpha and beta with conjuguate gradient.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Linear SVM\n",
    "\n",
    "# Compute linear kernel, L, Q\n",
    "Ker = Xtrain.dot(Xtrain.T)\n",
    "l = l_train\n",
    "L = np.diag(l)\n",
    "Q = L.dot(Ker.dot(L))\n",
    "\n",
    "# Time steps\n",
    "tau_alpha = 10/ np.linalg.norm(Q,2)\n",
    "tau_beta = 0.1/ np.linalg.norm(L,2)\n",
    "\n",
    "# For conjuguate gradient\n",
    "Acg = tau_alpha* Q + np.eye(n)\n",
    "\n",
    "# Pre-compute J.K(Xtest) for test data\n",
    "LKXtest = L.dot(Xtrain.dot(Xtest.T))\n",
    "\n",
    "# Initialization\n",
    "alpha_old = alpha\n",
    "\n",
    "# Loop\n",
    "k = 0\n",
    "diff_alpha = 1e6\n",
    "num_iter = 201\n",
    "while (diff_alpha>1e-3) & (k<num_iter):\n",
    "    \n",
    "    # Update iteration\n",
    "    k += 1\n",
    "    \n",
    "    # Update alpha\n",
    "    # Approximate solution with conjuguate gradient\n",
    "    b0 = alpha + tau_alpha - tau_alpha* l* beta\n",
    "    alpha, _ = scipy.sparse.linalg.cg(Acg, b0, x0=alpha, tol=1e-3, maxiter=50)   \n",
    "    \n",
    "    # Projection of alpha on [0,+infty]\n",
    "    ############################################################################\n",
    "    # Your code start\n",
    "    ############################################################################\n",
    "\n",
    "    alpha\n",
    "\n",
    "    ############################################################################\n",
    "    # Your code here\n",
    "    ############################################################################\n",
    "\n",
    "    # Update beta\n",
    "    beta = beta + tau_beta* l.T.dot(alpha)\n",
    "    \n",
    "    # Stopping condition\n",
    "    diff_alpha = np.linalg.norm(alpha-alpha_old)\n",
    "    alpha_old = alpha\n",
    "    \n",
    "    # Plot\n",
    "    if not(k%10) or (diff_alpha<1e-3):\n",
    "           \n",
    "        # Indicator function of support vectors\n",
    "        idx = np.where( np.abs(alpha)>0.25* np.max(np.abs(alpha)) )\n",
    "        Isv = np.zeros([n]); Isv[idx] = 1\n",
    "        nb_sv = len(Isv.nonzero()[0])\n",
    "        \n",
    "        # Offset\n",
    "        if nb_sv > 1:\n",
    "            b = (Isv.T).dot( l - Ker.dot(L.dot(alpha)) )/ nb_sv\n",
    "        else:\n",
    "            b = 0\n",
    "            \n",
    "        # Continuous score function\n",
    "        f_test = alpha.T.dot(LKXtest) + b \n",
    "\n",
    "        # Binary classification function\n",
    "        C_test = np.sign(f_test) # decision function in {-1,1}\n",
    "        accuracy_test = compute_purity(0.5*(1+C_test),Cgt_test,nc) # 0.5*(1+C_test) in {0,1}\n",
    "\n",
    "        # Plot\n",
    "        plt.figure(figsize=(8,4))\n",
    "        p1 = plt.subplot(121)\n",
    "        plt.scatter(Xtest[:,0], Xtest[:,1], s=size_vertex_plot*np.ones(n), c=f_test, color=pyplot.jet())\n",
    "        plt.title('Score function $s(x)=w^Tx+b$ \\n iter=' + str(k)+ ', diff_alpha=' + str(diff_alpha)[:7])\n",
    "        plt.colorbar()\n",
    "        p2 = plt.subplot(122)\n",
    "        plt.scatter(Xtest[:,0], Xtest[:,1], s=size_vertex_plot*np.ones(n), c=C_test, color=pyplot.jet())\n",
    "        plt.title('Classification function $f(x)=sign(w^Tx+b)$\\n iter=' + str(k) + ', acc=' + str(accuracy_test)[:5])\n",
    "        plt.tight_layout()\n",
    "        plt.colorbar()\n",
    "        plt.show()\n",
    "        if k<num_iter-1:\n",
    "            clear_output(wait=True)   \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-linearly separable data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "mat = scipy.io.loadmat('datasets/data_twomoons_softSVM.mat')\n",
    "Xtrain = mat['Xtrain']\n",
    "Cgt_train = mat['C_train_errors'] - 1; Cgt_train = Cgt_train.squeeze()\n",
    "Cgt_train[:250] = 0; Cgt_train[250:] = 1\n",
    "l_train = mat['l'].squeeze()\n",
    "n = Xtrain.shape[0]\n",
    "d = Xtrain.shape[1]\n",
    "nc = len(np.unique(Cgt_train))\n",
    "print(n,d,nc)\n",
    "Xtest = mat['Xtest']\n",
    "Cgt_test = mat['Cgt_test'] - 1; Cgt_test = Cgt_test.squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "plt.figure(figsize=(10,4))\n",
    "p1 = plt.subplot(121)\n",
    "size_vertex_plot = 33\n",
    "plt.scatter(Xtrain[:,0], Xtrain[:,1], s=size_vertex_plot*np.ones(n), c=Cgt_train, color=pyplot.jet())\n",
    "plt.title('Training Data')\n",
    "p2 = plt.subplot(122)\n",
    "size_vertex_plot = 33\n",
    "plt.scatter(Xtest[:,0], Xtest[:,1], s=size_vertex_plot*np.ones(n), c=Cgt_test, color=pyplot.jet())\n",
    "plt.title('Test Data')\n",
    "#plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2: Compute linear kernel, L, Q, time steps, initialization and projection of alpha as for Question 1\n",
    "\n",
    "Compare the results with the linearly separable case and determine which performs better. \n",
    "\n",
    "Answer: \n",
    "\n",
    "What strategy can be used to enhance the performance of SVM on non-linearly separable data?\n",
    "\n",
    "Answer: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Linear SVM\n",
    "\n",
    "# Compute linear kernel, L, Q\n",
    "Ker = # YOUR CODE HERE \n",
    "l = l_train\n",
    "L = # YOUR CODE HERE \n",
    "Q = # YOUR CODE HERE \n",
    "\n",
    "# Time steps\n",
    "tau_alpha = # YOUR CODE HERE \n",
    "tau_beta = # YOUR CODE HERE \n",
    "\n",
    "# For conjuguate gradient\n",
    "Acg = tau_alpha* Q + np.eye(n)\n",
    "\n",
    "# Pre-compute J.K(Xtest) for test data\n",
    "LKXtest = L.dot(Xtrain.dot(Xtest.T))\n",
    "\n",
    "# Initialization\n",
    "alpha = # YOUR CODE HERE \n",
    "beta = # YOUR CODE HERE \n",
    "alpha_old = alpha\n",
    "\n",
    "# Loop\n",
    "k = 0\n",
    "diff_alpha = 1e6\n",
    "num_iter = 201\n",
    "while (diff_alpha>1e-3) & (k<num_iter):\n",
    "    \n",
    "    # Update iteration\n",
    "    k += 1\n",
    "    \n",
    "    # Update alpha\n",
    "    # Approximate solution with conjuguate gradient\n",
    "    b0 = alpha + tau_alpha - tau_alpha* l* beta\n",
    "    alpha, _ = scipy.sparse.linalg.cg(Acg, b0, x0=alpha, tol=1e-3, maxiter=50)   \n",
    "\n",
    "    # Projection on [0,+infty]\n",
    "    alpha# YOUR CODE HERE  \n",
    "\n",
    "    # Update beta\n",
    "    beta = beta + tau_beta* l.T.dot(alpha)\n",
    "    \n",
    "    # Stopping condition\n",
    "    diff_alpha = np.linalg.norm(alpha-alpha_old)\n",
    "    alpha_old = alpha\n",
    "    \n",
    "    # Plot\n",
    "    if not(k%10) or (diff_alpha<1e-3):\n",
    "           \n",
    "        # Indicator function of support vectors\n",
    "        idx = np.where( np.abs(alpha)>0.25* np.max(np.abs(alpha)) )\n",
    "        Isv = np.zeros([n]); Isv[idx] = 1\n",
    "        nb_sv = len(Isv.nonzero()[0])\n",
    "        \n",
    "        # Offset\n",
    "        if nb_sv > 1:\n",
    "            b = (Isv.T).dot( l - Ker.dot(L.dot(alpha)) )/ nb_sv\n",
    "        else:\n",
    "            b = 0\n",
    "            \n",
    "        # Continuous score function\n",
    "        f_test = alpha.T.dot(LKXtest) + b \n",
    "\n",
    "        # Binary classification function\n",
    "        C_test = np.sign(f_test) # decision function in {-1,1}\n",
    "        accuracy_test = compute_purity(0.5*(1+C_test),Cgt_test,nc) # 0.5*(1+C_test) in {0,1}\n",
    "\n",
    "        # Plot\n",
    "        size_vertex_plot = 33\n",
    "        plt.figure(figsize=(12,4))\n",
    "        p1 = plt.subplot(121)\n",
    "        plt.scatter(Xtest[:,0], Xtest[:,1], s=size_vertex_plot*np.ones(n), c=f_test, color=pyplot.jet())\n",
    "        plt.title('Score function $s(x)=w^Tx+b$ \\n iter=' + str(k)+ ', diff_alpha=' + str(diff_alpha)[:7])\n",
    "        plt.colorbar()\n",
    "        p2 = plt.subplot(122)\n",
    "        plt.scatter(Xtest[:,0], Xtest[:,1], s=size_vertex_plot*np.ones(n), c=C_test, color=pyplot.jet())\n",
    "        plt.title('Classification function $f(x)=sign(w^Tx+b)$\\n iter=' + str(k) + ', acc=' + str(accuracy_test)[:5])\n",
    "        #plt.tight_layout()\n",
    "        plt.colorbar()\n",
    "        plt.show()\n",
    "        if k<num_iter-1:\n",
    "            clear_output(wait=True)   \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
