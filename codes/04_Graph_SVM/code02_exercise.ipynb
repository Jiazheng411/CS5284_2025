{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture : Graph SVM\n",
    "\n",
    "## Lab 02 : Soft-Margin SVM -- Exercise\n",
    "\n",
    "### Xavier Bresson, Guoji Fu \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Google Colaboratory\n",
    "import sys, os\n",
    "if 'google.colab' in sys.modules:\n",
    "    # mount google drive\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')\n",
    "    path_to_file = '/content/gdrive/My Drive/CS5284_2024_codes/codes/04_Graph_SVM'\n",
    "    print(path_to_file)\n",
    "    # change current path to the folder containing \"path_to_file\"\n",
    "    os.chdir(path_to_file)\n",
    "    !pwd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "%matplotlib inline\n",
    "#%matplotlib notebook \n",
    "from matplotlib import pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'figure.max_open_warning': 0})\n",
    "from IPython.display import display, clear_output\n",
    "import time\n",
    "import sys; sys.path.insert(0, 'lib/')\n",
    "from lib.utils import compute_purity\n",
    "from lib.utils import compute_SVM\n",
    "import warnings; warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linearly separable data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data matrix X = linearly separable data points\n",
    "mat = scipy.io.loadmat('datasets/data_softSVM.mat')\n",
    "Xtrain = mat['Xtrain']\n",
    "Cgt_train = mat['C_train_errors'] - 1; Cgt_train = Cgt_train.squeeze()\n",
    "l_train = mat['l'].squeeze()\n",
    "n = Xtrain.shape[0]\n",
    "d = Xtrain.shape[1]\n",
    "nc = len(np.unique(Cgt_train))\n",
    "print(n,d,nc)\n",
    "Xtest = mat['Xtest']\n",
    "Cgt_test = mat['Cgt_test'] - 1; Cgt_test = Cgt_test.squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "plt.figure(figsize=(8,4))\n",
    "p1 = plt.subplot(121)\n",
    "size_vertex_plot = 100\n",
    "plt.scatter(Xtrain[:,0], Xtrain[:,1], s=size_vertex_plot*np.ones(n), c=Cgt_train, color=pyplot.jet())\n",
    "plt.title('Training Data with 25% ERRORS')\n",
    "p2 = plt.subplot(122)\n",
    "size_vertex_plot = 100\n",
    "plt.scatter(Xtest[:,0], Xtest[:,1], s=size_vertex_plot*np.ones(n), c=Cgt_test, color=pyplot.jet())\n",
    "plt.title('Test Data')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: Evaluate the performance of soft-margin SVM on linearly separable data with different error parameters\n",
    "\n",
    "How does the performance of a soft-margin SVM change on training and testing data as the error parameter increases?\n",
    "\n",
    "Answer:  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error parameter\n",
    "############################################################################\n",
    "# Your code start\n",
    "############################################################################\n",
    "\n",
    "lamb = \n",
    "\n",
    "############################################################################\n",
    "# Your code end\n",
    "############################################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2: Project $\\alpha$ to $[0, \\lambda]$ during the update of alpha and beta with conjuguate gradient.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run soft-margin SVM\n",
    "\n",
    "# Compute linear kernel, L, Q\n",
    "Ker = Xtrain.dot(Xtrain.T)\n",
    "l = l_train\n",
    "L = np.diag(l)\n",
    "Q = L.dot(Ker.dot(L))\n",
    "\n",
    "# Time steps\n",
    "tau_alpha = 10/ np.linalg.norm(Q,2)\n",
    "tau_beta = 0.1/ np.linalg.norm(L,2)\n",
    "\n",
    "# For conjuguate gradient\n",
    "Acg = tau_alpha* Q + np.eye(n)\n",
    "\n",
    "# Pre-compute J.K(Xtest) for test data\n",
    "LKXtest = L.dot(Xtrain.dot(Xtest.T))\n",
    "\n",
    "# Initialization\n",
    "alpha = np.zeros([n])\n",
    "beta = np.ones([n])\n",
    "alpha_old = alpha\n",
    "\n",
    "# Loop\n",
    "k = 0\n",
    "diff_alpha = 1e6\n",
    "num_iter = 201\n",
    "while (diff_alpha>1e-3) & (k<num_iter):\n",
    "    \n",
    "    # Update iteration\n",
    "    k += 1\n",
    "    \n",
    "    # Update alpha\n",
    "    # Approximate solution with conjuguate gradient\n",
    "    b0 = alpha + tau_alpha - tau_alpha* l* beta\n",
    "    alpha, _ = scipy.sparse.linalg.cg(Acg, b0, x0=alpha, tol=1e-3, maxiter=50)  \n",
    "    \n",
    "    # Projection of alpha on [0, \\lambda]\n",
    "    ############################################################################\n",
    "    # Your code start\n",
    "    ############################################################################\n",
    "\n",
    "    alpha\n",
    "\n",
    "    ############################################################################\n",
    "    # Your code end\n",
    "    ############################################################################\n",
    "\n",
    "    # Update beta\n",
    "    beta = beta + tau_beta* l.T.dot(alpha)\n",
    "    \n",
    "    # Stopping condition\n",
    "    diff_alpha = np.linalg.norm(alpha-alpha_old)\n",
    "    alpha_old = alpha\n",
    "    \n",
    "    # Plot\n",
    "    if not(k%10) or (diff_alpha<1e-3):\n",
    "           \n",
    "        # Indicator function of support vectors\n",
    "        idx = np.where( np.abs(alpha)>0.25* np.max(np.abs(alpha)) )\n",
    "        Isv = np.zeros([n]); Isv[idx] = 1\n",
    "        nb_sv = len(Isv.nonzero()[0])\n",
    "        \n",
    "        # Offset\n",
    "        if nb_sv > 1:\n",
    "            b = (Isv.T).dot( l - Ker.dot(L.dot(alpha)) )/ nb_sv\n",
    "        else:\n",
    "            b = 0\n",
    "            \n",
    "        # Continuous score function\n",
    "        f_test = alpha.T.dot(LKXtest) + b \n",
    "\n",
    "        # Binary classification function\n",
    "        C_test = np.sign(f_test) # decision function in {-1,1}\n",
    "        accuracy_test = compute_purity(0.5*(1+C_test),Cgt_test,nc) # 0.5*(1+C_test) in {0,1}\n",
    "\n",
    "        # Plot\n",
    "        plt.figure(figsize=(8,4))\n",
    "        p1 = plt.subplot(121)\n",
    "        plt.scatter(Xtest[:,0], Xtest[:,1], s=size_vertex_plot*np.ones(n), c=f_test, color=pyplot.jet())\n",
    "        plt.title('Score function $s(x)=w^Tx+b$ \\n iter=' + str(k)+ ', diff_alpha=' + str(diff_alpha)[:7])\n",
    "        plt.colorbar()\n",
    "        p2 = plt.subplot(122)\n",
    "        plt.scatter(Xtest[:,0], Xtest[:,1], s=size_vertex_plot*np.ones(n), c=C_test, color=pyplot.jet())\n",
    "        plt.title('Classification function $f(x)=sign(w^Tx+b)$\\n iter=' + str(k) + ', acc=' + str(accuracy_test)[:5])\n",
    "        plt.tight_layout()\n",
    "        plt.colorbar()\n",
    "        plt.show()\n",
    "        if k<num_iter-1:\n",
    "            clear_output(wait=True)   \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-linearly separable data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "mat = scipy.io.loadmat('datasets/data_twomoons_softSVM.mat')\n",
    "Xtrain = mat['Xtrain']\n",
    "Cgt_train = mat['C_train_errors'] - 1; Cgt_train = Cgt_train.squeeze()\n",
    "l_train = mat['l'].squeeze()\n",
    "n = Xtrain.shape[0]\n",
    "d = Xtrain.shape[1]\n",
    "nc = len(np.unique(Cgt_train))\n",
    "print(n,d,nc)\n",
    "Xtest = mat['Xtest']\n",
    "Cgt_test = mat['Cgt_test'] - 1; Cgt_test = Cgt_test.squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "plt.figure(figsize=(10,4))\n",
    "p1 = plt.subplot(121)\n",
    "size_vertex_plot = 33\n",
    "plt.scatter(Xtrain[:,0], Xtrain[:,1], s=size_vertex_plot*np.ones(n), c=Cgt_train, color=pyplot.jet())\n",
    "plt.title('Training Data with 25% ERRORS')\n",
    "p2 = plt.subplot(122)\n",
    "size_vertex_plot = 33\n",
    "plt.scatter(Xtest[:,0], Xtest[:,1], s=size_vertex_plot*np.ones(n), c=Cgt_test, color=pyplot.jet())\n",
    "plt.title('Test Data')\n",
    "#plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3: Evaluate the performance of soft-margin SVM on non-linearly separable data with different error parameters\n",
    "\n",
    "Compare the results with hard-margin Linear SVM, can significant improvements in soft-margin linear SVM over hard-margin linear SVM be achieved by tuning the error parameter on non-linearly separable data?\n",
    "\n",
    "Answer: \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run soft-margin SVM\n",
    "\n",
    "# Compute linear kernel, L, Q\n",
    "Ker = Xtrain.dot(Xtrain.T)\n",
    "l = l_train\n",
    "L = np.diag(l)\n",
    "Q = L.dot(Ker.dot(L))\n",
    "\n",
    "# Time steps\n",
    "tau_alpha = 10/ np.linalg.norm(Q,2)\n",
    "tau_beta = 0.1/ np.linalg.norm(L,2)\n",
    "\n",
    "# For conjuguate gradient\n",
    "Acg = tau_alpha* Q + np.eye(n)\n",
    "\n",
    "# Pre-compute J.K(Xtest) for test data\n",
    "LKXtest = L.dot(Xtrain.dot(Xtest.T))\n",
    "\n",
    "# Error parameter\n",
    "############################################################################\n",
    "# Your code start\n",
    "############################################################################\n",
    "\n",
    "lamb = \n",
    "\n",
    "############################################################################\n",
    "# Your code end\n",
    "############################################################################\n",
    "\n",
    "# Initialization\n",
    "alpha = np.zeros([n])\n",
    "beta = 0.0\n",
    "alpha_old = alpha\n",
    "\n",
    "# Loop\n",
    "k = 0\n",
    "diff_alpha = 1e6\n",
    "num_iter = 201\n",
    "while (diff_alpha>1e-3) & (k<num_iter):\n",
    "    \n",
    "    # Update iteration\n",
    "    k += 1\n",
    "    \n",
    "    # Update alpha\n",
    "    # Approximate solution with conjuguate gradient\n",
    "    b0 = alpha + tau_alpha - tau_alpha* l* beta\n",
    "    alpha, _ = scipy.sparse.linalg.cg(Acg, b0, x0=alpha, tol=1e-3, maxiter=50)   \n",
    "    alpha[alpha<0.0] = 0 # Projection on [0,+infty]\n",
    "    alpha[alpha>lamb] = lamb # Projection on [-infty,lamb]\n",
    "\n",
    "    # Update beta\n",
    "    beta = beta + tau_beta* l.T.dot(alpha)\n",
    "    \n",
    "    # Stopping condition\n",
    "    diff_alpha = np.linalg.norm(alpha-alpha_old)\n",
    "    alpha_old = alpha\n",
    "    \n",
    "    # Plot\n",
    "    if not(k%10) or (diff_alpha<1e-3):\n",
    "           \n",
    "        # Indicator function of support vectors\n",
    "        idx = np.where( np.abs(alpha)>0.25* np.max(np.abs(alpha)) )\n",
    "        Isv = np.zeros([n]); Isv[idx] = 1\n",
    "        nb_sv = len(Isv.nonzero()[0])\n",
    "        \n",
    "        # Offset\n",
    "        if nb_sv > 1:\n",
    "            b = (Isv.T).dot( l - Ker.dot(L.dot(alpha)) )/ nb_sv\n",
    "        else:\n",
    "            b = 0\n",
    "            \n",
    "        # Continuous score function\n",
    "        f_test = alpha.T.dot(LKXtest) + b \n",
    "\n",
    "        # Binary classification function\n",
    "        C_test = np.sign(f_test) # decision function in {-1,1}\n",
    "        accuracy_test = compute_purity(0.5*(1+C_test),Cgt_test,nc) # 0.5*(1+C_test) in {0,1}\n",
    "\n",
    "        # Plot\n",
    "        size_vertex_plot = 33\n",
    "        plt.figure(figsize=(12,4))\n",
    "        p1 = plt.subplot(121)\n",
    "        plt.scatter(Xtest[:,0], Xtest[:,1], s=size_vertex_plot*np.ones(n), c=f_test, color=pyplot.jet())\n",
    "        plt.title('Score function $s(x)=w^Tx+b$ \\n iter=' + str(k)+ ', diff_alpha=' + str(diff_alpha)[:7])\n",
    "        plt.colorbar()\n",
    "        p2 = plt.subplot(122)\n",
    "        plt.scatter(Xtest[:,0], Xtest[:,1], s=size_vertex_plot*np.ones(n), c=C_test, color=pyplot.jet())\n",
    "        plt.title('Classification function $f(x)=sign(w^Tx+b)$\\n iter=' + str(k) + ', acc=' + str(accuracy_test)[:5])\n",
    "        #plt.tight_layout()\n",
    "        plt.colorbar()\n",
    "        plt.show()\n",
    "        if k<num_iter-1:\n",
    "            clear_output(wait=True)   \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
