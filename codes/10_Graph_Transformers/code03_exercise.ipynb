{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LfFWcbbExXcc"
   },
   "source": [
    "# Lecture : Graph Transformers & Graph ViT\n",
    "\n",
    "## Lab 03 : Graph Transformers with edge features and DGL (sparse linear algebra) -- Exercise\n",
    "\n",
    "### Xavier Bresson, Guoji Fu\n",
    "\n",
    "Dwivedi, Bresson, A generalization of transformer networks to graphs, 2020   \n",
    "https://arxiv.org/pdf/2012.09699.pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7102,
     "status": "ok",
     "timestamp": 1730637248755,
     "user": {
      "displayName": "Guoji Fu",
      "userId": "16398754709610840055"
     },
     "user_tz": -480
    },
    "id": "IZCvd1fTxXce",
    "outputId": "41ceed4f-96e9-4b1a-a395-f72c3621d79d"
   },
   "outputs": [],
   "source": [
    "# For Google Colaboratory\n",
    "import sys, os\n",
    "if 'google.colab' in sys.modules:\n",
    "    # mount google drive\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')\n",
    "    path_to_file = '/content/gdrive/My Drive/CS5284_2025_codes/codes/10_Graph_Transformers'\n",
    "    print(path_to_file)\n",
    "    # change current path to the folder containing \"path_to_file\"\n",
    "    os.chdir(path_to_file)\n",
    "    !pwd\n",
    "    !pip install dgl==1.0.0 # Install DGL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Y9hiy25BxXcf"
   },
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import pickle\n",
    "import dgl\n",
    "import dgl.function as fn\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import networkx as nx\n",
    "import sys; sys.path.insert(0, 'lib/')\n",
    "from lib.utils import compute_ncut\n",
    "from lib.molecules import Dictionary, MoleculeDataset, MoleculeDGL, Molecule\n",
    "import os, datetime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x3Es6_zDxXcf"
   },
   "source": [
    "# Load molecular datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4351,
     "status": "ok",
     "timestamp": 1730637253097,
     "user": {
      "displayName": "Guoji Fu",
      "userId": "16398754709610840055"
     },
     "user_tz": -480
    },
    "id": "fl68-dJTxXcf",
    "outputId": "7f142b87-0e09-4434-d76b-18d370ec5f8e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "4\n",
      "Loading datasets QM9_1.4k_dgl...\n",
      "train, test, val sizes : 1000 200 200\n",
      "Time: 2.0298s\n",
      "1000\n",
      "200\n",
      "200\n",
      "([Graph(num_nodes=9, num_edges=20,\n",
      "      ndata_schemes={'feat': Scheme(shape=(), dtype=torch.int64)}\n",
      "      edata_schemes={'feat': Scheme(shape=(), dtype=torch.int64)}), Graph(num_nodes=9, num_edges=18,\n",
      "      ndata_schemes={'feat': Scheme(shape=(), dtype=torch.int64)}\n",
      "      edata_schemes={'feat': Scheme(shape=(), dtype=torch.int64)})], [tensor([-0.2532]), tensor([1.0897])])\n",
      "(Graph(num_nodes=9, num_edges=18,\n",
      "      ndata_schemes={'feat': Scheme(shape=(), dtype=torch.int64)}\n",
      "      edata_schemes={'feat': Scheme(shape=(), dtype=torch.int64)}), tensor([0.5060]))\n",
      "(Graph(num_nodes=9, num_edges=20,\n",
      "      ndata_schemes={'feat': Scheme(shape=(), dtype=torch.int64)}\n",
      "      edata_schemes={'feat': Scheme(shape=(), dtype=torch.int64)}), tensor([-4.4048]))\n"
     ]
    }
   ],
   "source": [
    "# Select dataset\n",
    "dataset_name = 'QM9_1.4k'; data_folder_pytorch = 'dataset/QM9_1.4k_pytorch/'; data_folder_dgl = 'dataset/QM9_1.4k_dgl/'\n",
    "\n",
    "# Load the number of atom and bond types\n",
    "with open(data_folder_pytorch + \"atom_dict.pkl\" ,\"rb\") as f: num_atom_type = len(pickle.load(f))\n",
    "with open(data_folder_pytorch + \"bond_dict.pkl\" ,\"rb\") as f: num_bond_type = len(pickle.load(f))\n",
    "print(num_atom_type)\n",
    "print(num_bond_type)\n",
    "\n",
    "# Load the DGL datasets\n",
    "datasets_dgl = MoleculeDataset(dataset_name, data_folder_dgl)\n",
    "trainset, valset, testset = datasets_dgl.train, datasets_dgl.val, datasets_dgl.test\n",
    "print(len(trainset))\n",
    "print(len(valset))\n",
    "print(len(testset))\n",
    "idx = 0\n",
    "print(trainset[:2])\n",
    "print(valset[idx])\n",
    "print(testset[idx])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OTYjHxtUxXcg"
   },
   "source": [
    "# Add positional encoding feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1998,
     "status": "ok",
     "timestamp": 1730637255093,
     "user": {
      "displayName": "Guoji Fu",
      "userId": "16398754709610840055"
     },
     "user_tz": -480
    },
    "id": "TQp3RdpAxXcg",
    "outputId": "19126433-8031-4aa7-d982-1cd2d516a142"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Graph(num_nodes=9, num_edges=20,\n",
      "      ndata_schemes={'feat': Scheme(shape=(), dtype=torch.int64), 'pos_enc': Scheme(shape=(3,), dtype=torch.float32)}\n",
      "      edata_schemes={'feat': Scheme(shape=(), dtype=torch.int64)}), tensor([-0.2532]))\n"
     ]
    }
   ],
   "source": [
    "# Positional encoding as Laplacian eigenvectors\n",
    "def LapEig_positional_encoding(g, pos_enc_dim):\n",
    "    Adj = g.adj().to_dense() # Adjacency matrix\n",
    "    Dn = ( g.in_degrees()** -0.5 ).diag() # Inverse and sqrt of degree matrix\n",
    "    Lap = torch.eye(g.number_of_nodes()) - Dn.matmul(Adj).matmul(Dn) # Laplacian operator\n",
    "    EigVal, EigVec = torch.linalg.eig(Lap) # Compute full EVD\n",
    "    EigVal, EigVec = EigVal.real, EigVec.real # make eig real\n",
    "    EigVec = EigVec[:, EigVal.argsort()] # sort in increasing order of eigenvalues\n",
    "    EigVec = EigVec[:,1:pos_enc_dim+1] # select the first non-trivial \"pos_enc_dim\" eigenvector\n",
    "    return EigVec\n",
    "\n",
    "# Add node positional encoding features to graphs\n",
    "pos_enc_dim = 3 # dimension of PE, QM9\n",
    "def add_node_edge_features(dataset):\n",
    "    for (graph,_) in dataset:\n",
    "        graph.ndata['pos_enc'] = LapEig_positional_encoding(graph, pos_enc_dim) # node positional encoding feature\n",
    "    return dataset\n",
    "\n",
    "# Generate graph datasets\n",
    "trainset = add_node_edge_features(trainset)\n",
    "testset = add_node_edge_features(testset)\n",
    "valset = add_node_edge_features(valset)\n",
    "print(trainset[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ukrFXYNexXcg"
   },
   "source": [
    "# Visualize positional encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 863
    },
    "executionInfo": {
     "elapsed": 819,
     "status": "ok",
     "timestamp": 1730637255907,
     "user": {
      "displayName": "Guoji Fu",
      "userId": "16398754709610840055"
     },
     "user_tz": -480
    },
    "id": "i77JNkdBxXcg",
    "outputId": "a2e5b1cf-7e62-4fe9-8653-d1fb55bab668"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "graph = trainset[0][0]\n",
    "\n",
    "# Visualize graph\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "Adj = graph.adj().to_dense()\n",
    "A_nx = nx.from_numpy_array(Adj.numpy())\n",
    "C = compute_ncut(Adj.long(), 4)\n",
    "nx.draw(A_nx, ax=ax, node_color=C, cmap='jet', with_labels=True, font_size=10) # visualise node indexes\n",
    "ax.title.set_text('Visualization with networkx')\n",
    "plt.show()\n",
    "\n",
    "# plot 2D coordinates\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "x = graph.ndata['pos_enc']\n",
    "ax.scatter(x[:,0], x[:,1])\n",
    "idx = list(range(graph.number_of_nodes()))\n",
    "ax.scatter(x[:,0], x[:,1], c=C, cmap='jet')\n",
    "for i, txt in enumerate(idx):\n",
    "    ax.annotate(txt, (x[:,0][i], x[:,1][i]), textcoords=\"offset points\", xytext=(1,5))\n",
    "ax.title.set_text('2D embdding of nodes')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R14LMFIRxXch"
   },
   "source": [
    "# Define the collate function to prepare a batch of DGL graphs and test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 689,
     "status": "ok",
     "timestamp": 1730637256594,
     "user": {
      "displayName": "Guoji Fu",
      "userId": "16398754709610840055"
     },
     "user_tz": -480
    },
    "id": "d7ZXaxwKxXch",
    "outputId": "af206596-e7cd-41ce-9b51-20b2b9e4167e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph(num_nodes=88, num_edges=190,\n",
      "      ndata_schemes={'feat': Scheme(shape=(), dtype=torch.int64), 'pos_enc': Scheme(shape=(3,), dtype=torch.float32)}\n",
      "      edata_schemes={'feat': Scheme(shape=(), dtype=torch.int64)})\n",
      "tensor([[ 0.7261],\n",
      "        [ 3.9831],\n",
      "        [ 0.9307],\n",
      "        [-1.0135],\n",
      "        [-1.5031],\n",
      "        [ 0.5415],\n",
      "        [-0.6457],\n",
      "        [ 0.1768],\n",
      "        [-0.7758],\n",
      "        [-0.0883]])\n",
      "batch_x: torch.Size([88])\n",
      "batch_pe: torch.Size([88, 3])\n",
      "batch_e: torch.Size([190])\n"
     ]
    }
   ],
   "source": [
    "# collate function prepares a batch of graphs, labels and other graph features (if needed)\n",
    "def collate(samples):\n",
    "    # Input sample is a list of pairs (graph, label)\n",
    "    graphs, labels = map(list, zip(*samples))\n",
    "    batch_graphs = dgl.batch(graphs)    # batch of graphs\n",
    "    batch_labels = torch.stack(labels)  # batch of labels (here chemical target)\n",
    "    return batch_graphs, batch_labels\n",
    "\n",
    "\n",
    "# Generate a batch of graphs\n",
    "batch_size = 10\n",
    "train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True, collate_fn=collate)\n",
    "batch_graphs, batch_labels = list(train_loader)[0]\n",
    "print(batch_graphs)\n",
    "print(batch_labels)\n",
    "batch_x = batch_graphs.ndata['feat']\n",
    "print('batch_x:',batch_x.size())\n",
    "batch_pe = batch_graphs.ndata['pos_enc']\n",
    "print('batch_pe:',batch_pe.size())\n",
    "batch_e = batch_graphs.edata['feat']\n",
    "print('batch_e:',batch_e.size())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7VBKKjxWxXch"
   },
   "source": [
    "# Exercise 1: Design the class of GraphTransformer networks with edge features\n",
    "\n",
    "## Node Update Equations\n",
    "\n",
    "$$\\bar{h}^{\\ell} = h^{\\ell} + \\textrm{gMHA} (\\textrm{LN}(h^{\\ell}),\\textrm{LN}(e^{\\ell})) \\in \\mathbb{R}^{N\\times d}$$\n",
    "\n",
    "$$h^{\\ell+1} = \\bar{h}^{\\ell} + \\textrm{MLP} (\\textrm{LN}(\\bar{h}^{\\ell})) \\in \\mathbb{R}^{N\\times d}$$\n",
    "\n",
    "**Graph Multi-Head Attention:**\n",
    "$$\\textrm{gMHA}(h,e)=\\textrm{Concat}_{k=1}^H \\left( \\textrm{gHA}(h_k,e_k) \\right) W_O \\in \\mathbb{R}^{N\\times d}$$\n",
    "\n",
    "where $h_k\\in \\mathbb{R}^{N\\times d'=d/H}$, $e_k\\in \\mathbb{R}^{E\\times d'}$, $W_O\\in \\mathbb{R}^{d\\times d}$\n",
    "\n",
    "**Graph Attention (point-wise equation):**\n",
    "$$\\textrm{gHA}(h,e)_i= \\sum_{j\\in \\mathcal{N}_i} \\underbrace{\\frac{\\exp(q_i^T \\textrm{diag}(e_{ij}) k_j/\\sqrt{d'})}{ \\sum_{j'\\in\\mathcal{N}_i} \\exp(q_i^T \\textrm{diag}(e_{ij'}) k_{j'}/\\sqrt{d'}) }}_{\\textrm{graph attention score w/ edge feature}_{ij}} v_j$$\n",
    "\n",
    "**Linear Transformations:**\n",
    "$$Q=h_k W_Q, \\quad K=h_k W_K, \\quad V=h_k W_V\\in \\mathbb{R}^{N\\times d'=d/H}$$\n",
    "\n",
    "$$E=e_k W_E\\in \\mathbb{R}^{E\\times d'=d/H}$$\n",
    "\n",
    "where $W_Q, W_K, W_V, W_E\\in \\mathbb{R}^{d'\\times d'}$\n",
    "\n",
    "**Initial Node Features:**\n",
    "$$h^{\\ell=0} = \\textrm{LL}_1(h_0)+\\textrm{LL}_2(p_0) \\in \\mathbb{R}^{N\\times d}$$\n",
    "\n",
    "**Positional Encoding:**\n",
    "$$p_0=\\Phi_{\\{2,..,K+1\\}}\\in \\mathbb{R}^{N\\times K}, \\quad \\Delta = \\Phi \\Lambda \\Phi^T \\in \\mathbb{R}^{N\\times N}$$\n",
    "\n",
    "## Edge Update Equations\n",
    "\n",
    "$$\\bar{e}^{\\ell} = e^{\\ell} + \\textrm{gMHE} (\\textrm{LN}(e^{\\ell}),\\textrm{LN}(h^{\\ell})) \\in \\mathbb{R}^{E\\times d}$$\n",
    "\n",
    "$$e^{\\ell+1} = \\bar{e}^{\\ell} + \\textrm{MLP} (\\textrm{LN}(\\bar{e}^{\\ell})) \\in \\mathbb{R}^{E\\times d}$$\n",
    "\n",
    "**Graph Multi-Head Edge Attention:**\n",
    "$$\\textrm{gMHE}(e,h)=\\textrm{Concat}_{k=1}^H \\left( \\textrm{gHE}(e_k,h_k) \\right) W_O^e \\in \\mathbb{R}^{E\\times d}$$\n",
    "\n",
    "where $h_k\\in \\mathbb{R}^{N\\times d'=d/H}$, $e_k\\in \\mathbb{R}^{E\\times d'}$, $W_O^e\\in \\mathbb{R}^{d\\times d}$\n",
    "\n",
    "**Graph Head Edge (point-wise equation):**\n",
    "$$\\textrm{gHE}(e,h)_{ij}=q_i \\odot e_{ij} \\odot k_j/\\sqrt{d'} \\in \\mathbb{R}^{d'}$$\n",
    "\n",
    "**Initial Edge Features:**\n",
    "$$e^{\\ell=0} = \\textrm{LL}(e_0) \\in \\mathbb{R}^{E\\times d}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MqI7VXd9gchf"
   },
   "source": [
    "### Question 1.1: Implement a Graph Multi-Head Attention (MHA) Layer with edge features\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "- *Step 1 of message-passing with DGL:* Pass node feature and edge features along edges (src/j => dst/i) by:\n",
    "    - *Step 1.1:* Compute bi-linear products with edge feature: $q_i^T * diag(e_{ij}) * k_j$. You may use ```edges.dst[]``` for ```i, edges.src[]``` for ```j, edges.data[]``` form ```ij```\".  \n",
    "\n",
    "    - *Step 1.2* Compute $\\textrm{exp}_{ij} = \\exp( q_i^T * k_j / \\sqrt{d'} )$, ```size=(E,K,1)```.\n",
    "\n",
    "    - *Step 1.3:* Obtain ```V```.\n",
    "\n",
    "    - *Step 1.4:* Compute edge feature: $q_i^T * diag(e_{ij}) * k_j$.\n",
    "\n",
    "    - *Step 1.5:* Update edge feature.\n",
    "\n",
    "- *Step 2 of message-passing with DGL:* Define a reduce function that\n",
    "    - *Step 2.1:* Use ```nodes.mailbox[]``` to collects all messages ```= {vj, eij}``` sent to node dst/i with *Step 1*.\n",
    "    \n",
    "    - *Step 2.2:* Sum/mean over the graph neigbors ```j``` in ```Ni```.\n",
    "\n",
    "- Assign ```Q, K, V, E, F, G```  to graphs by storing them in the ndata dictionary with the keys ```'Q', 'K', 'V', 'E', 'F', 'G'``` for ```g.ndata[]``` and reshape them using ```.view(-1, num_heads, head_hidden_dim)```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "XQ-OgcDJ7rC4"
   },
   "outputs": [],
   "source": [
    "# class graph multi head attention layer\n",
    "class graph_MHA_layer(nn.Module): # MHA = Multi Head Attention\n",
    "\n",
    "    def __init__(self, hidden_dim, head_hidden_dim, num_heads): # hidden_dim = d\n",
    "        super().__init__()\n",
    "        self.head_hidden_dim = head_hidden_dim # head_hidden_dim = d' = d/K\n",
    "        self.num_heads = num_heads # number of heads = K\n",
    "        self.WQ = nn.Linear(hidden_dim, head_hidden_dim * num_heads, bias=True) # define K x W matrix of size=(d',d')\n",
    "        self.WK = nn.Linear(hidden_dim, head_hidden_dim * num_heads, bias=True)\n",
    "        self.WV = nn.Linear(hidden_dim, head_hidden_dim * num_heads, bias=True)\n",
    "        self.WE = nn.Linear(hidden_dim, head_hidden_dim * num_heads, bias=True)\n",
    "        self.WF = nn.Linear(hidden_dim, head_hidden_dim * num_heads, bias=True)\n",
    "        self.WG = nn.Linear(hidden_dim, head_hidden_dim * num_heads, bias=True)\n",
    "\n",
    "    # Step 1 of message-passing with DGL:\n",
    "    #   Node feature and edge features are passed along edges (src/j => dst/i)\n",
    "    def message_func(self, edges):\n",
    "        ###############################################\n",
    "        # YOUR CODE STARTS\n",
    "        ###############################################\n",
    "        # Step 1.1: Compute bi-linear products with edge feature\n",
    "        qi = edges.dst['Q']  # size=(E,K,d')\n",
    "        kj = edges.src['K']  # size=(E,K,d')\n",
    "        ej = edges.data['E']  # size=(E,K,d')\n",
    "        qikj = ( kj * ej * qi).sum(dim=2).unsqueeze(2)  # size=(E,K,1)\n",
    "\n",
    "        # Step 1.2: Compute exp_ij = exp( q_i^T * k_j / sqrt(d') ), size=(E,K,1)\n",
    "        expij =  torch.exp( qikj / torch.sqrt(torch.tensor(self.head_hidden_dim)) )\n",
    "\n",
    "        # Step 1.3: Obtain vj\n",
    "        vj = edges.src['V']  # size=(E,K,d')\n",
    "\n",
    "        # Step 1.4: Compute edge feature: e_ij = q_i^T * diag(E_ij) * k_j / sqrt(d'), size=(E,K,d')\n",
    "        eij = kj * ej * qi / torch.sqrt(torch.tensor(self.head_hidden_dim))\n",
    "\n",
    "\n",
    "        # Step 1.5: Update edge feature\n",
    "        edges.data['e'] = eij\n",
    "        ###############################################\n",
    "        # YOUR CODE ENDS\n",
    "        ###############################################\n",
    "\n",
    "        return {'expij' : expij, 'vj' : vj}\n",
    "\n",
    "    # Step 2 of message-passing with DGL:\n",
    "    #   Reduce function collects all messages={hj, eij} sent to node dst/i with Step 1\n",
    "    #                   and sum/mean over the graph neigbors j in Ni\n",
    "    def reduce_func(self, nodes):\n",
    "        ###############################################\n",
    "        # YOUR CODE STARTS\n",
    "        ###############################################\n",
    "        # Step 2.1: Collects all messages= eij\n",
    "        # size=(N,|Nj|,K,1), |Nj|=num_neighbors\n",
    "        expij = nodes.mailbox['expij']  # size=(N,|Nj|,K,1)\n",
    "\n",
    "        # Step 2.1: Collects all messages= vj\n",
    "        # size=(N,|Nj|,K,d')\n",
    "        vj = nodes.mailbox['vj']  # size=(N,|Nj|,K,d')\n",
    "\n",
    "        # Step 2.2: Sum/mean over the graph neigbors j in Ni\n",
    "        # sum_j exp_ij . v_j, size=(N,K,d')\n",
    "        numerator = torch.sum( expij * vj, dim=1 )  # size=(N,K,d')\n",
    "\n",
    "        # sum_j' exp_ij', size=(N,K,1)\n",
    "        denominator = torch.sum( expij, dim=1 )  # size=(N,K,1)\n",
    "\n",
    "        # h_i = sum_j score_ij . v_j , where score_ij = exp_ij / sum_j' exp_ij', size=(N,K,d')\n",
    "        h = numerator / denominator\n",
    "        ###############################################\n",
    "        # YOUR CODE ENDS\n",
    "        ###############################################\n",
    "\n",
    "        return {'h' : h}\n",
    "\n",
    "    def forward(self, g, h, e):\n",
    "        Q = self.WQ(h) # size=(N, d)\n",
    "                       # computational trick to compute quickly K linear transformations h_k.WQ of size=(N, d')\n",
    "                       # first compute linear transformation h.WQ of size=(N, d)\n",
    "                       # then reshape h.WQ of size=(N, K, d'=d/K)\n",
    "        K = self.WK(h) # size=(N, d)\n",
    "        V = self.WV(h) # size=(N, d)\n",
    "        E = self.WE(e) # size=(E, d)\n",
    "        # F = self.WF(h) # size=(N, d)\n",
    "        # G = self.WG(h) # size=(N, d)\n",
    "\n",
    "        ###############################################\n",
    "        # YOUR CODE STARTS\n",
    "        ###############################################\n",
    "        g.ndata['Q'] = Q.view(-1, self.num_heads, self.head_hidden_dim) # size=(N, K, d'=d/K)\n",
    "        g.ndata['K'] = K.view(-1, self.num_heads, self.head_hidden_dim) # size=(N, K, d'=d/K)\n",
    "        g.ndata['V'] = V.view(-1, self.num_heads, self.head_hidden_dim) # size=(N, K, d'=d/K)\n",
    "        g.edata['E'] = E.view(-1, self.num_heads, self.head_hidden_dim) # size=(E, K, d'=d/K)\n",
    "        # g.ndata['F'] = F.view(-1, self.num_heads, self.head_hidden_dim) # size=(N, K, d'=d/K)\n",
    "        # g.ndata['G'] = G.view(-1, self.num_heads, self.head_hidden_dim) # size=(N, K, d'=d/K)\n",
    "        g.update_all(self.message_func, self.reduce_func) # compute with DGL the graph MHA\n",
    "        ###############################################\n",
    "        # YOUR CODE ENDS\n",
    "        ###############################################\n",
    "\n",
    "        gMHA = g.ndata['h'] # size=(N, K, d'=d/K)\n",
    "        gMHE = g.edata['e'] # size=(E, K, d'=d/K)\n",
    "        return gMHA, gMHE\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sJix5TqbrFhR"
   },
   "source": [
    "### Question 1.2: Implement a Graph Transformer layer (with edge feature)\n",
    "\n",
    "- Implement dropout, layer normalization, and residual connection layers for edge features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Roe4rKU371mH"
   },
   "outputs": [],
   "source": [
    "# class GraphTransformer layer\n",
    "class GraphTransformer_layer(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_dim, num_heads, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim # hidden_dim = d\n",
    "        self.num_heads = num_heads # number of heads = K\n",
    "        self.dropout_h_mha = nn.Dropout(dropout) # dropout value\n",
    "        self.dropout_h_mlp = nn.Dropout(dropout) # dropout value\n",
    "        self.gMHA = graph_MHA_layer(hidden_dim, hidden_dim//num_heads, num_heads) # graph MHA layer\n",
    "        self.WO = nn.Linear(hidden_dim, hidden_dim) # LL\n",
    "        self.layer_norm1 = nn.LayerNorm(hidden_dim) # layer normalization\n",
    "        self.layer_norm1e = nn.LayerNorm(hidden_dim)\n",
    "        self.linear1 = nn.Linear(hidden_dim, hidden_dim) # LL1 for MLP\n",
    "        self.linear2 = nn.Linear(hidden_dim, hidden_dim) # LL2 for MLP\n",
    "\n",
    "        ###############################################\n",
    "        # YOUR CODE STARTS\n",
    "        ###############################################\n",
    "        # Dropout layers for edge features\n",
    "        self.dropout_e_mha =    nn.Dropout(dropout)\n",
    "        self.dropout_e_mlp =    nn.Dropout(dropout)\n",
    "\n",
    "        # MLP layers for edge features\n",
    "        self.WOe =   nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "        # Layer normalization for edge features\n",
    "        self.layer_norm2 =   nn.LayerNorm(hidden_dim)\n",
    "        self.layer_norm2e =  nn.LayerNorm(hidden_dim)\n",
    "\n",
    "        # MLP layers for edge features\n",
    "        self.linear1e =   nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.linear2e =   nn.Linear(hidden_dim, hidden_dim)\n",
    "        ###############################################\n",
    "        # YOUR CODE ENDS\n",
    "        ###############################################\n",
    "\n",
    "    def forward(self, g, h, e):\n",
    "\n",
    "        # Self-attention layer\n",
    "        h_rc = h # size=(N,d), V=num_nodes, for residual connection\n",
    "        e_rc = e\n",
    "        h = self.layer_norm1(h) # layer normalization, size=(N, d)\n",
    "        ###############################################\n",
    "        # YOUR CODE STARTS\n",
    "        ###############################################\n",
    "        # layer normalization for edge features, size=(N, d)\n",
    "        e = self.layer_norm1e(e)\n",
    "        ###############################################\n",
    "        # YOUR CODE ENDS\n",
    "        ###############################################\n",
    "\n",
    "        h_MHA, e_MHE = self.gMHA(g, h, e) # MHA, size=(N, K, d'=d/K)\n",
    "        h_MHA = h_MHA.view(-1, self.hidden_dim) # size=(N, d)\n",
    "        h_MHA = self.dropout_h_mha(h_MHA) # dropout, size=(N, d)\n",
    "        h_MHA = self.WO(h_MHA) # LL, size=(N, d)\n",
    "        h = h_rc + h_MHA # residual connection, size=(N, d)\n",
    "\n",
    "        ###############################################\n",
    "        # YOUR CODE STARTS\n",
    "        ###############################################\n",
    "        # Update for edge features\n",
    "        e_MHE = e_MHE.view(-1, self.hidden_dim) # size=(N, d)\n",
    "        e_MHE = self.dropout_e_mha(e_MHE) # dropout, size=(N, d)\n",
    "        e_MHE = self.WOe(e_MHE) # LL, size=(N, d)\n",
    "        e = e_rc + e_MHE # residual connection, size=(N, d)\n",
    "        ###############################################\n",
    "        # YOUR CODE ENDS\n",
    "        ###############################################\n",
    "\n",
    "        # Fully-connected layer\n",
    "        h_rc = h # for residual connection, size=(N, d)\n",
    "        e_rc = e # for residual connection, size=(N, d)\n",
    "        h = self.layer_norm2(h) # layer normalization, size=(N, d)\n",
    "        e = self.layer_norm2e(e) # layer normalization, size=(N, d)\n",
    "        h_MLP = self.linear1(h) # LL, size=(H, d)\n",
    "        e_MLP = self.linear1e(e) # LL, size=(H, d)\n",
    "        h_MLP = torch.relu(h_MLP) # size=(N, d)\n",
    "        e_MLP = torch.relu(e_MLP) # size=(N, d)\n",
    "        h_MLP = self.dropout_h_mlp(h_MLP) # dropout, size=(N, d)\n",
    "        e_MLP = self.dropout_e_mlp(e_MLP) # dropout, size=(N, d)\n",
    "        h_MLP = self.linear2(h_MLP) # LL, size=(N, d)\n",
    "        e_MLP = self.linear2e(e_MLP) # LL, size=(N, d)\n",
    "        h = h_rc + h_MLP # residual connection, size=(N, d)\n",
    "        e = e_rc + e_MLP # residual connection, size=(N, d)\n",
    "\n",
    "        return h, e\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rST8IpB2rYqK"
   },
   "source": [
    "### Question 1.3: Combine all previous defined MLP Layer, GraphTransformer layer to construct the Graph Transformer network (with edge feature)\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "- *Adding a input edge embedding layer:* Initialize a linear layer ```nn.Linear()``` to convert input edge features into edge embeddings.\n",
    "\n",
    "- *Graph transformer layer (with edge feature):* Initialize a ModuleList ```nn.ModuleList()``` containing ```L``` instances of ```GraphTransformer_layer()```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "2DQRl2bYxXch"
   },
   "outputs": [],
   "source": [
    "# class Graph Transformer network\n",
    "class GraphTransformer_net(nn.Module):\n",
    "\n",
    "    def __init__(self, net_parameters):\n",
    "        super(GraphTransformer_net, self).__init__()\n",
    "        input_dim = net_parameters['input_dim']\n",
    "        pos_enc_dim = net_parameters['pos_enc_dim']\n",
    "        hidden_dim = net_parameters['hidden_dim']\n",
    "        num_heads = net_parameters['num_heads']\n",
    "        L = net_parameters['L']\n",
    "        self.embedding_h = nn.Embedding(num_atom_type, hidden_dim)\n",
    "\n",
    "        ###############################################\n",
    "        # YOUR CODE STARTS\n",
    "        ###############################################\n",
    "        # Initialize a edge embedding layer\n",
    "        self.embedding_e = nn.Embedding(num_bond_type, hidden_dim)\n",
    "        ###############################################\n",
    "        # YOUR CODE ENDS\n",
    "        ###############################################\n",
    "\n",
    "        self.embedding_pe = nn.Linear(pos_enc_dim, hidden_dim)\n",
    "        self.GraphTransformer_layers = nn.ModuleList([ GraphTransformer_layer(hidden_dim, num_heads) for _ in range(L) ])\n",
    "        self.ln_h_final = nn.LayerNorm(hidden_dim)\n",
    "        self.linear_h_final = nn.Linear(hidden_dim, 1, bias=True)\n",
    "\n",
    "    def forward(self, g, h, pe, e):\n",
    "\n",
    "        # input node embedding\n",
    "        h = self.embedding_h(h) # size=(num_nodes, hidden_dim)\n",
    "\n",
    "        # if PE used\n",
    "        # h = h + self.embedding_pe(pe) # size=(num_nodes, hidden_dim)\n",
    "\n",
    "        ###############################################\n",
    "        # YOUR CODE STARTS\n",
    "        ###############################################\n",
    "        # Implement teh edge embedding layer\n",
    "        # size=(num_edges, hidden_dim)\n",
    "        e =  self.embedding_e(e)\n",
    "        ###############################################\n",
    "        # YOUR CODE ENDS\n",
    "        ###############################################\n",
    "\n",
    "        # graph convnet layers\n",
    "        for GT_layer in self.GraphTransformer_layers:\n",
    "            h, e = GT_layer(g, h, e) # size=(num_nodes, hidden_dim)\n",
    "\n",
    "        # MLP classifier\n",
    "        g.ndata['h'] = h\n",
    "        mol_token = dgl.mean_nodes(g,'h') # DGL mean function over the neighbors, size=(num_graphs, hidden_dim)\n",
    "        y = self.ln_h_final(mol_token)\n",
    "        y = self.linear_h_final(y) # size=(num_graphs, num_classes)\n",
    "\n",
    "        return y\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1730637256594,
     "user": {
      "displayName": "Guoji Fu",
      "userId": "16398754709610840055"
     },
     "user_tz": -480
    },
    "id": "qzWVgqKW7vSU",
    "outputId": "3703d3bb-d48e-43d6-ed06-8723e11cb904"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GraphTransformer_net(\n",
      "  (embedding_h): Embedding(9, 128)\n",
      "  (embedding_e): Embedding(4, 128)\n",
      "  (embedding_pe): Linear(in_features=3, out_features=128, bias=True)\n",
      "  (GraphTransformer_layers): ModuleList(\n",
      "    (0-3): 4 x GraphTransformer_layer(\n",
      "      (dropout_h_mha): Dropout(p=0.0, inplace=False)\n",
      "      (dropout_h_mlp): Dropout(p=0.0, inplace=False)\n",
      "      (gMHA): graph_MHA_layer(\n",
      "        (WQ): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (WK): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (WV): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (WE): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (WF): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (WG): Linear(in_features=128, out_features=128, bias=True)\n",
      "      )\n",
      "      (WO): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (layer_norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (layer_norm1e): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (linear1): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (linear2): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (dropout_e_mha): Dropout(p=0.0, inplace=False)\n",
      "      (dropout_e_mlp): Dropout(p=0.0, inplace=False)\n",
      "      (WOe): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (layer_norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (layer_norm2e): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (linear1e): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (linear2e): Linear(in_features=128, out_features=128, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (ln_h_final): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (linear_h_final): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n",
      "torch.Size([10, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\64483\\miniconda3\\envs\\gnn_course\\lib\\site-packages\\dgl\\backend\\pytorch\\tensor.py:445: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  assert input.numel() == input.storage().size(), (\n"
     ]
    }
   ],
   "source": [
    "# Instantiate one network (testing)\n",
    "net_parameters = {}\n",
    "net_parameters['input_dim'] = 1\n",
    "net_parameters['pos_enc_dim'] = pos_enc_dim\n",
    "net_parameters['hidden_dim'] = 128\n",
    "net_parameters['num_heads'] = 8\n",
    "net_parameters['L'] = 4\n",
    "net = GraphTransformer_net(net_parameters)\n",
    "print(net)\n",
    "\n",
    "batch_size = 10\n",
    "train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True, collate_fn=collate)\n",
    "batch_graphs, batch_labels = list(train_loader)[0]\n",
    "batch_x = batch_graphs.ndata['feat']\n",
    "batch_pe = batch_graphs.ndata['pos_enc']\n",
    "batch_e = batch_graphs.edata['feat']\n",
    "batch_labels = batch_labels\n",
    "batch_scores = net(batch_graphs, batch_x, batch_pe, batch_e)\n",
    "print(batch_scores.size())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G0gZzixzxXci"
   },
   "source": [
    "# Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 291348,
     "status": "ok",
     "timestamp": 1730637547939,
     "user": {
      "displayName": "Guoji Fu",
      "userId": "16398754709610840055"
     },
     "user_tz": -480
    },
    "id": "Hw7LEG5exXci",
    "outputId": "ead6b872-7372-4f0c-cffa-845c8147baff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 799233 (0.80 million)\n",
      "Epoch 0, time 2.4974, train_loss: 1.2665, test_loss: 1.1502\n",
      "Epoch 0, time 2.4974, train_loss: 1.2665, test_loss: 1.1502\n",
      "Epoch 1, time 5.1551, train_loss: 1.0734, test_loss: 1.0504\n",
      "Epoch 1, time 5.1551, train_loss: 1.0734, test_loss: 1.0504\n",
      "Epoch 2, time 7.8008, train_loss: 1.0152, test_loss: 0.9611\n",
      "Epoch 2, time 7.8008, train_loss: 1.0152, test_loss: 0.9611\n",
      "Epoch 3, time 10.3952, train_loss: 1.0302, test_loss: 1.0893\n",
      "Epoch 3, time 10.3952, train_loss: 1.0302, test_loss: 1.0893\n",
      "Epoch 4, time 12.9463, train_loss: 0.9560, test_loss: 0.9514\n",
      "Epoch 4, time 12.9463, train_loss: 0.9560, test_loss: 0.9514\n",
      "Epoch 5, time 15.5155, train_loss: 0.9303, test_loss: 0.9302\n",
      "Epoch 5, time 15.5155, train_loss: 0.9303, test_loss: 0.9302\n",
      "Epoch 6, time 18.0082, train_loss: 0.9492, test_loss: 0.9387\n",
      "Epoch 6, time 18.0082, train_loss: 0.9492, test_loss: 0.9387\n",
      "Epoch 7, time 20.5780, train_loss: 0.9131, test_loss: 0.8965\n",
      "Epoch 7, time 20.5780, train_loss: 0.9131, test_loss: 0.8965\n",
      "Epoch 8, time 23.0110, train_loss: 0.8791, test_loss: 0.9028\n",
      "Epoch 8, time 23.0110, train_loss: 0.8791, test_loss: 0.9028\n",
      "Epoch 9, time 25.5485, train_loss: 0.8763, test_loss: 0.8730\n",
      "Epoch 9, time 25.5485, train_loss: 0.8763, test_loss: 0.8730\n",
      "Epoch 10, time 27.9898, train_loss: 0.8488, test_loss: 0.8975\n",
      "Epoch 10, time 27.9898, train_loss: 0.8488, test_loss: 0.8975\n",
      "Epoch 11, time 30.4612, train_loss: 0.8863, test_loss: 0.8663\n",
      "Epoch 11, time 30.4612, train_loss: 0.8863, test_loss: 0.8663\n",
      "Epoch 12, time 32.8401, train_loss: 0.8729, test_loss: 0.8789\n",
      "Epoch 12, time 32.8401, train_loss: 0.8729, test_loss: 0.8789\n",
      "Epoch 13, time 35.4400, train_loss: 0.8417, test_loss: 0.8291\n",
      "Epoch 13, time 35.4400, train_loss: 0.8417, test_loss: 0.8291\n",
      "Epoch 14, time 37.9383, train_loss: 0.8134, test_loss: 0.8073\n",
      "Epoch 14, time 37.9383, train_loss: 0.8134, test_loss: 0.8073\n",
      "Epoch 15, time 40.4842, train_loss: 0.8081, test_loss: 0.8147\n",
      "Epoch 15, time 40.4842, train_loss: 0.8081, test_loss: 0.8147\n",
      "Epoch 16, time 42.9571, train_loss: 0.7929, test_loss: 0.8625\n",
      "Epoch 16, time 42.9571, train_loss: 0.7929, test_loss: 0.8625\n",
      "Epoch 17, time 45.4233, train_loss: 0.7642, test_loss: 0.7733\n",
      "Epoch 17, time 45.4233, train_loss: 0.7642, test_loss: 0.7733\n",
      "Epoch 18, time 47.8908, train_loss: 0.7705, test_loss: 0.7867\n",
      "Epoch 18, time 47.8908, train_loss: 0.7705, test_loss: 0.7867\n",
      "Epoch 19, time 50.4414, train_loss: 0.7654, test_loss: 0.7680\n",
      "Epoch 19, time 50.4414, train_loss: 0.7654, test_loss: 0.7680\n",
      "Epoch 20, time 52.9344, train_loss: 0.7829, test_loss: 0.8119\n",
      "Epoch 20, time 52.9344, train_loss: 0.7829, test_loss: 0.8119\n",
      "Epoch 21, time 55.6460, train_loss: 0.7445, test_loss: 0.7533\n",
      "Epoch 21, time 55.6460, train_loss: 0.7445, test_loss: 0.7533\n",
      "Epoch 22, time 58.2574, train_loss: 0.7464, test_loss: 0.7497\n",
      "Epoch 22, time 58.2574, train_loss: 0.7464, test_loss: 0.7497\n",
      "Epoch 23, time 60.7886, train_loss: 0.7269, test_loss: 0.7548\n",
      "Epoch 23, time 60.7886, train_loss: 0.7269, test_loss: 0.7548\n",
      "Epoch 24, time 63.3279, train_loss: 0.7284, test_loss: 0.8382\n",
      "Epoch 24, time 63.3279, train_loss: 0.7284, test_loss: 0.8382\n",
      "Epoch 25, time 65.9314, train_loss: 0.7045, test_loss: 0.7959\n",
      "Epoch 25, time 65.9314, train_loss: 0.7045, test_loss: 0.7959\n",
      "Epoch 26, time 68.9762, train_loss: 0.7115, test_loss: 0.7480\n",
      "Epoch 26, time 68.9762, train_loss: 0.7115, test_loss: 0.7480\n",
      "Epoch 27, time 71.6437, train_loss: 0.7013, test_loss: 0.7460\n",
      "Epoch 27, time 71.6437, train_loss: 0.7013, test_loss: 0.7460\n",
      "Epoch 28, time 74.6759, train_loss: 0.7138, test_loss: 0.8105\n",
      "Epoch 28, time 74.6759, train_loss: 0.7138, test_loss: 0.8105\n",
      "Epoch 29, time 77.0490, train_loss: 0.6930, test_loss: 0.7640\n",
      "Epoch 29, time 77.0490, train_loss: 0.6930, test_loss: 0.7640\n",
      "Epoch 30, time 79.6263, train_loss: 0.7019, test_loss: 0.8879\n",
      "Epoch 30, time 79.6263, train_loss: 0.7019, test_loss: 0.8879\n",
      "Epoch 31, time 82.5931, train_loss: 0.7634, test_loss: 0.7951\n",
      "Epoch 31, time 82.5931, train_loss: 0.7634, test_loss: 0.7951\n",
      "Epoch 32, time 85.2172, train_loss: 0.6773, test_loss: 0.7681\n",
      "Epoch 32, time 85.2172, train_loss: 0.6773, test_loss: 0.7681\n",
      "Epoch 33, time 87.7942, train_loss: 0.7031, test_loss: 0.7594\n",
      "Epoch 33, time 87.7942, train_loss: 0.7031, test_loss: 0.7594\n",
      "Epoch 34, time 90.3194, train_loss: 0.6679, test_loss: 0.7535\n",
      "Epoch 34, time 90.3194, train_loss: 0.6679, test_loss: 0.7535\n",
      "Epoch 35, time 92.7126, train_loss: 0.6635, test_loss: 0.7546\n",
      "Epoch 35, time 92.7126, train_loss: 0.6635, test_loss: 0.7546\n",
      "Epoch 36, time 95.1075, train_loss: 0.6551, test_loss: 0.7704\n",
      "Epoch 36, time 95.1075, train_loss: 0.6551, test_loss: 0.7704\n",
      "Epoch 37, time 97.5960, train_loss: 0.6785, test_loss: 0.7853\n",
      "Epoch 37, time 97.5960, train_loss: 0.6785, test_loss: 0.7853\n",
      "Epoch 38, time 100.0702, train_loss: 0.6593, test_loss: 0.7402\n",
      "Epoch 38, time 100.0702, train_loss: 0.6593, test_loss: 0.7402\n",
      "Epoch 39, time 102.4419, train_loss: 0.6291, test_loss: 0.7687\n",
      "Epoch 39, time 102.4419, train_loss: 0.6291, test_loss: 0.7687\n",
      "Epoch 40, time 104.8838, train_loss: 0.6428, test_loss: 0.7583\n",
      "Epoch 40, time 104.8838, train_loss: 0.6428, test_loss: 0.7583\n",
      "Epoch 41, time 107.2337, train_loss: 0.6540, test_loss: 0.7607\n",
      "Epoch 41, time 107.2337, train_loss: 0.6540, test_loss: 0.7607\n",
      "Epoch 42, time 109.6776, train_loss: 0.6272, test_loss: 0.7630\n",
      "Epoch 42, time 109.6776, train_loss: 0.6272, test_loss: 0.7630\n",
      "Epoch 43, time 112.0473, train_loss: 0.6552, test_loss: 0.7706\n",
      "Epoch 43, time 112.0473, train_loss: 0.6552, test_loss: 0.7706\n",
      "Epoch 44, time 114.5272, train_loss: 0.6194, test_loss: 0.7655\n",
      "Epoch 44, time 114.5272, train_loss: 0.6194, test_loss: 0.7655\n",
      "Epoch 45, time 117.0699, train_loss: 0.6095, test_loss: 0.7321\n",
      "Epoch 45, time 117.0699, train_loss: 0.6095, test_loss: 0.7321\n",
      "Epoch 46, time 119.4671, train_loss: 0.6153, test_loss: 0.7436\n",
      "Epoch 46, time 119.4671, train_loss: 0.6153, test_loss: 0.7436\n",
      "Epoch 47, time 121.9433, train_loss: 0.6143, test_loss: 0.7496\n",
      "Epoch 47, time 121.9433, train_loss: 0.6143, test_loss: 0.7496\n",
      "Epoch 48, time 124.4558, train_loss: 0.6225, test_loss: 0.7794\n",
      "Epoch 48, time 124.4558, train_loss: 0.6225, test_loss: 0.7794\n",
      "Epoch 49, time 126.8539, train_loss: 0.6147, test_loss: 0.7275\n",
      "Epoch 49, time 126.8539, train_loss: 0.6147, test_loss: 0.7275\n",
      "Epoch 50, time 129.2452, train_loss: 0.6020, test_loss: 0.7498\n",
      "Epoch 50, time 129.2452, train_loss: 0.6020, test_loss: 0.7498\n",
      "Epoch 51, time 131.6980, train_loss: 0.5908, test_loss: 0.7407\n",
      "Epoch 51, time 131.6980, train_loss: 0.5908, test_loss: 0.7407\n",
      "Epoch 52, time 134.2857, train_loss: 0.5948, test_loss: 0.7505\n",
      "Epoch 52, time 134.2857, train_loss: 0.5948, test_loss: 0.7505\n",
      "Epoch 53, time 136.8533, train_loss: 0.5917, test_loss: 0.7440\n",
      "Epoch 53, time 136.8533, train_loss: 0.5917, test_loss: 0.7440\n",
      "Epoch 54, time 139.1993, train_loss: 0.5751, test_loss: 0.7303\n",
      "Epoch 54, time 139.1993, train_loss: 0.5751, test_loss: 0.7303\n",
      "Epoch 55, time 141.5633, train_loss: 0.5891, test_loss: 0.7771\n",
      "Epoch 55, time 141.5633, train_loss: 0.5891, test_loss: 0.7771\n",
      "Epoch 56, time 143.9494, train_loss: 0.5943, test_loss: 0.7385\n",
      "Epoch 56, time 143.9494, train_loss: 0.5943, test_loss: 0.7385\n",
      "Epoch 57, time 146.3289, train_loss: 0.5897, test_loss: 0.7880\n",
      "Epoch 57, time 146.3289, train_loss: 0.5897, test_loss: 0.7880\n",
      "Epoch 58, time 148.9244, train_loss: 0.6089, test_loss: 0.7379\n",
      "Epoch 58, time 148.9244, train_loss: 0.6089, test_loss: 0.7379\n",
      "Epoch 59, time 151.4147, train_loss: 0.5848, test_loss: 0.7398\n",
      "Epoch 59, time 151.4147, train_loss: 0.5848, test_loss: 0.7398\n",
      "Epoch 60, time 153.7303, train_loss: 0.5840, test_loss: 0.7304\n",
      "Epoch 60, time 153.7303, train_loss: 0.5840, test_loss: 0.7304\n",
      "Epoch 61, time 156.1358, train_loss: 0.5589, test_loss: 0.7441\n",
      "Epoch 61, time 156.1358, train_loss: 0.5589, test_loss: 0.7441\n",
      "Epoch 62, time 158.5240, train_loss: 0.5549, test_loss: 0.7032\n",
      "Epoch 62, time 158.5240, train_loss: 0.5549, test_loss: 0.7032\n",
      "Epoch 63, time 160.8573, train_loss: 0.5551, test_loss: 0.7334\n",
      "Epoch 63, time 160.8573, train_loss: 0.5551, test_loss: 0.7334\n",
      "Epoch 64, time 163.3880, train_loss: 0.5603, test_loss: 0.7460\n",
      "Epoch 64, time 163.3880, train_loss: 0.5603, test_loss: 0.7460\n",
      "Epoch 65, time 165.6707, train_loss: 0.5711, test_loss: 0.7454\n",
      "Epoch 65, time 165.6707, train_loss: 0.5711, test_loss: 0.7454\n",
      "Epoch 66, time 168.0457, train_loss: 0.5468, test_loss: 0.8167\n",
      "Epoch 66, time 168.0457, train_loss: 0.5468, test_loss: 0.8167\n",
      "Epoch 67, time 170.4187, train_loss: 0.5720, test_loss: 0.7351\n",
      "Epoch 67, time 170.4187, train_loss: 0.5720, test_loss: 0.7351\n",
      "Epoch 68, time 173.0120, train_loss: 0.5478, test_loss: 0.7470\n",
      "Epoch 68, time 173.0120, train_loss: 0.5478, test_loss: 0.7470\n",
      "Epoch 69, time 175.5544, train_loss: 0.5498, test_loss: 0.7720\n",
      "Epoch 69, time 175.5544, train_loss: 0.5498, test_loss: 0.7720\n",
      "Epoch 70, time 177.9104, train_loss: 0.5373, test_loss: 0.7314\n",
      "Epoch 70, time 177.9104, train_loss: 0.5373, test_loss: 0.7314\n",
      "Epoch 71, time 180.5215, train_loss: 0.5267, test_loss: 0.7522\n",
      "Epoch 71, time 180.5215, train_loss: 0.5267, test_loss: 0.7522\n",
      "Epoch 72, time 182.8757, train_loss: 0.5657, test_loss: 0.7434\n",
      "Epoch 72, time 182.8757, train_loss: 0.5657, test_loss: 0.7434\n",
      "Epoch 73, time 185.2940, train_loss: 0.5442, test_loss: 0.6918\n",
      "Epoch 73, time 185.2940, train_loss: 0.5442, test_loss: 0.6918\n",
      "Epoch 74, time 187.6699, train_loss: 0.5263, test_loss: 0.7222\n",
      "Epoch 74, time 187.6699, train_loss: 0.5263, test_loss: 0.7222\n",
      "Epoch 75, time 190.0815, train_loss: 0.5075, test_loss: 0.6905\n",
      "Epoch 75, time 190.0815, train_loss: 0.5075, test_loss: 0.6905\n",
      "Epoch 76, time 192.4908, train_loss: 0.5118, test_loss: 0.7406\n",
      "Epoch 76, time 192.4908, train_loss: 0.5118, test_loss: 0.7406\n",
      "Epoch 77, time 194.8931, train_loss: 0.5408, test_loss: 0.7373\n",
      "Epoch 77, time 194.8931, train_loss: 0.5408, test_loss: 0.7373\n",
      "Epoch 78, time 197.3901, train_loss: 0.5024, test_loss: 0.7526\n",
      "Epoch 78, time 197.3901, train_loss: 0.5024, test_loss: 0.7526\n",
      "Epoch 79, time 199.7896, train_loss: 0.5113, test_loss: 0.7235\n",
      "Epoch 79, time 199.7896, train_loss: 0.5113, test_loss: 0.7235\n",
      "Epoch 80, time 202.3467, train_loss: 0.5083, test_loss: 0.7266\n",
      "Epoch 80, time 202.3467, train_loss: 0.5083, test_loss: 0.7266\n",
      "Epoch 81, time 204.6957, train_loss: 0.5163, test_loss: 0.7092\n",
      "Epoch 81, time 204.6957, train_loss: 0.5163, test_loss: 0.7092\n",
      "Epoch 82, time 206.9982, train_loss: 0.5138, test_loss: 0.7491\n",
      "Epoch 82, time 206.9982, train_loss: 0.5138, test_loss: 0.7491\n",
      "Epoch 83, time 209.4290, train_loss: 0.5103, test_loss: 0.7157\n",
      "Epoch 83, time 209.4290, train_loss: 0.5103, test_loss: 0.7157\n",
      "Epoch 84, time 211.7645, train_loss: 0.4929, test_loss: 0.7669\n",
      "Epoch 84, time 211.7645, train_loss: 0.4929, test_loss: 0.7669\n",
      "Epoch 85, time 214.2499, train_loss: 0.5109, test_loss: 0.6960\n",
      "Epoch 85, time 214.2499, train_loss: 0.5109, test_loss: 0.6960\n",
      "Epoch 86, time 216.8562, train_loss: 0.4838, test_loss: 0.7308\n",
      "Epoch 86, time 216.8562, train_loss: 0.4838, test_loss: 0.7308\n",
      "Epoch 87, time 219.3090, train_loss: 0.5085, test_loss: 0.7466\n",
      "Epoch 87, time 219.3090, train_loss: 0.5085, test_loss: 0.7466\n",
      "Epoch 88, time 221.9001, train_loss: 0.5016, test_loss: 0.7126\n",
      "Epoch 88, time 221.9001, train_loss: 0.5016, test_loss: 0.7126\n",
      "Epoch 89, time 224.4592, train_loss: 0.5254, test_loss: 0.7200\n",
      "Epoch 89, time 224.4592, train_loss: 0.5254, test_loss: 0.7200\n",
      "Epoch 90, time 226.9238, train_loss: 0.4869, test_loss: 0.7084\n",
      "Epoch 90, time 226.9238, train_loss: 0.4869, test_loss: 0.7084\n",
      "Epoch 91, time 229.3982, train_loss: 0.4670, test_loss: 0.7276\n",
      "Epoch 91, time 229.3982, train_loss: 0.4670, test_loss: 0.7276\n",
      "Epoch 92, time 231.9537, train_loss: 0.4790, test_loss: 0.7264\n",
      "Epoch 92, time 231.9537, train_loss: 0.4790, test_loss: 0.7264\n",
      "Epoch 93, time 234.4728, train_loss: 0.4792, test_loss: 0.7363\n",
      "Epoch 93, time 234.4728, train_loss: 0.4792, test_loss: 0.7363\n",
      "Epoch 94, time 237.0739, train_loss: 0.4669, test_loss: 0.7333\n",
      "Epoch 94, time 237.0739, train_loss: 0.4669, test_loss: 0.7333\n",
      "Epoch 95, time 239.6513, train_loss: 0.4587, test_loss: 0.7215\n",
      "Epoch 95, time 239.6513, train_loss: 0.4587, test_loss: 0.7215\n",
      "Epoch 96, time 242.2132, train_loss: 0.4850, test_loss: 0.7227\n",
      "Epoch 96, time 242.2132, train_loss: 0.4850, test_loss: 0.7227\n",
      "Epoch 97, time 244.6853, train_loss: 0.4886, test_loss: 0.7008\n",
      "Epoch 97, time 244.6853, train_loss: 0.4886, test_loss: 0.7008\n",
      "Epoch 98, time 247.1583, train_loss: 0.4617, test_loss: 0.7186\n",
      "Epoch 98, time 247.1583, train_loss: 0.4617, test_loss: 0.7186\n",
      "Epoch 99, time 249.6137, train_loss: 0.4762, test_loss: 0.7021\n",
      "Epoch 99, time 249.6137, train_loss: 0.4762, test_loss: 0.7021\n"
     ]
    }
   ],
   "source": [
    "def run_one_epoch(net, data_loader, train=True, loss_fc=None, optimizer=None):\n",
    "    if train:\n",
    "        net.train() # during training\n",
    "    else:\n",
    "        net.eval()  # during inference/test\n",
    "    epoch_loss = 0\n",
    "    nb_data = 0\n",
    "    for iter, (batch_graphs, batch_labels) in enumerate(data_loader):\n",
    "        batch_x = batch_graphs.ndata['feat']\n",
    "        bs2 = batch_labels.size(0)\n",
    "        batch_pe = batch_graphs.ndata['pos_enc']\n",
    "        batch_pe = batch_pe * ( 2 * torch.randint(low=0, high=2, size=(1,pos_enc_dim)).float() - 1.0 ) # randomly flip sign of eigenvectors\n",
    "        batch_e = batch_graphs.edata['feat']\n",
    "        batch_labels = batch_labels\n",
    "        batch_scores = net.forward(batch_graphs, batch_x, batch_pe, batch_e)\n",
    "        lossMAE = loss_fc(batch_scores, batch_labels)\n",
    "        if train: # during training, run backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            lossMAE.backward()\n",
    "            optimizer.step()\n",
    "        epoch_loss += bs2 * lossMAE.detach().item()\n",
    "        nb_data += bs2\n",
    "    epoch_loss /= nb_data\n",
    "    return epoch_loss, optimizer\n",
    "\n",
    "\n",
    "# dataset loaders\n",
    "batch_size = 50\n",
    "train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True, collate_fn=collate)\n",
    "test_loader = DataLoader(testset, batch_size=batch_size, shuffle=False, collate_fn=collate)\n",
    "val_loader = DataLoader(valset, batch_size=batch_size, shuffle=False, drop_last=False, collate_fn=collate)\n",
    "\n",
    "# Instantiate one network\n",
    "net_parameters = {}\n",
    "net_parameters['input_dim'] = 1\n",
    "net_parameters['pos_enc_dim'] = pos_enc_dim\n",
    "net_parameters['hidden_dim'] = 128\n",
    "net_parameters['num_heads'] = 8\n",
    "net_parameters['L'] = 4\n",
    "del net\n",
    "net = GraphTransformer_net(net_parameters)\n",
    "def display_num_param(net):\n",
    "    nb_param = 0\n",
    "    for param in net.parameters():\n",
    "        nb_param += param.numel()\n",
    "    print('Number of parameters: {} ({:.2f} million)'.format(nb_param, nb_param/1e6))\n",
    "    return nb_param/1e6\n",
    "_ = display_num_param(net)\n",
    "\n",
    "# loss, optimizer\n",
    "lossMAE = nn.L1Loss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.0003)\n",
    "# optimizer = torch.optim.Adam(net.parameters(), lr=0.0001)\n",
    "\n",
    "# training loop\n",
    "start = time.time()\n",
    "for epoch in range(100):\n",
    "    epoch_train_loss, optimizer = run_one_epoch(net, train_loader, True, lossMAE, optimizer)\n",
    "    with torch.no_grad():\n",
    "        epoch_test_loss = run_one_epoch(net, test_loader, False, lossMAE)[0]\n",
    "        # epoch_val_loss = run_one_epoch(net, val_loader, False, lossMAE)[0]\n",
    "    print('Epoch {}, time {:.4f}, train_loss: {:.4f}, test_loss: {:.4f}'.format(epoch, time.time()-start, epoch_train_loss, epoch_test_loss))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ehCjHW1YxXci"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Amo_4YiKxXci"
   },
   "source": [
    "# GT without edge features\n",
    "\n",
    "Node update equation:\n",
    "\n",
    "$$\\bar{h}^{\\ell} = h^{\\ell} + \\textrm{gMHA} (\\textrm{LN}(h^{\\ell})) \\in \\mathbb{R}^{N\\times d}$$\n",
    "$$h^{\\ell+1} = \\bar{h}^{\\ell} + \\textrm{MLP} (\\textrm{LN}(\\bar{h}^{\\ell})) \\in \\mathbb{R}^{N\\times d}$$\n",
    "$$\\textrm{with } \\textrm{gMHA}(h)=\\textrm{Concat}_{k=1}^H \\left( \\textrm{gHA}(h_k) \\right) W_O \\in \\mathbb{R}^{N\\times d},\\ h_k\\in \\mathbb{R}^{N\\times d'=d/H}, W_O\\in \\mathbb{R}^{d\\times d} $$\n",
    "$$\\quad\\quad\\ \\textrm{gHA}(h)=\\textrm{Softmax}\\left( A_G \\odot \\frac{QK^T}{\\sqrt{d'}} \\right) V \\in \\mathbb{R}^{N\\times d'=d/H}, A_G\\in \\mathbb{R}^{N\\times N} \\textrm{ (graph adjacency matrix)}$$\n",
    "$$\\quad\\quad\\ \\textrm{gHA}(h)_i= \\sum_{j\\in \\mathcal{N}_i} \\underbrace{\\frac{\\exp(q_i^T k_j/\\sqrt{d'})}{ \\sum_{j'\\in\\mathcal{N}_i} \\exp(q_i^T k_{j'}/\\sqrt{d'}) }}_{\\textrm{graph attention score}_{ij}} v_j\\ \\textrm{ (point-wise equation)}$$\n",
    "$$\\quad\\quad\\ Q=h_k W_Q, K=h_k W_K, V=h_k W_V\\in \\mathbb{R}^{N\\times d'=d/H}, W_Q, W_K, W_V\\in \\mathbb{R}^{d'\\times d'}$$\n",
    "$$ h^{\\ell=0} = \\textrm{LL}_1(h_0)+\\textrm{LL}_2(p_0) \\in \\mathbb{R}^{N\\times d}\\ \\textrm{(input node feature and positional encoding)}$$\n",
    "$$\\textrm{with } p_0=\\Phi_{\\{2,..,K+1\\}}\\in \\mathbb{R}^{N\\times K},\\ \\Delta = \\Phi \\Lambda \\Phi^T \\in \\mathbb{R}^{N\\times N}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "BKceFO_W8HBf"
   },
   "outputs": [],
   "source": [
    "# class graph multi head attention layer\n",
    "class graph_MHA_layer(nn.Module): # MHA = Multi Head Attention\n",
    "\n",
    "    def __init__(self, hidden_dim, head_hidden_dim, num_heads): # hidden_dim = d\n",
    "        super().__init__()\n",
    "        self.head_hidden_dim = head_hidden_dim # head_hidden_dim = d' = d/K\n",
    "        self.num_heads = num_heads # number of heads = K\n",
    "        self.WQ = nn.Linear(hidden_dim, head_hidden_dim * num_heads, bias=True) # define K x WQ matrix of size=(d',d')\n",
    "        self.WK = nn.Linear(hidden_dim, head_hidden_dim * num_heads, bias=True)\n",
    "        self.WV = nn.Linear(hidden_dim, head_hidden_dim * num_heads, bias=True)\n",
    "\n",
    "    # Step 1 of message-passing with DGL:\n",
    "    #   Node feature and edge features are passed along edges (src/j => dst/i)\n",
    "    def message_func(self, edges):\n",
    "        # Compute the dot products q_i^T * k_j\n",
    "        # You may use \"edges.dst[] for i, edges.src[] for j\"\n",
    "        qikj = (edges.dst['Q'] * edges.src['K']).sum(dim=2).unsqueeze(2) # all dot products q_i^T * k_j, size=(E,K,1), edges.src/dst[].size=(E,K,d')\n",
    "        #qikj = ### YOUR CODE HERE, size=(E,K,1), , edges.src/dst[].size=(E,K,d')\n",
    "        expij = torch.exp( qikj / torch.sqrt(torch.tensor(self.head_hidden_dim)) ) # exp_ij = exp( clamp(q_i^T * k_j / sqrt(d')) ), size=(E,K,1)\n",
    "        vj = edges.src['V'] # size=(E,K,d')\n",
    "        return {'expij' : expij, 'vj' : vj}\n",
    "\n",
    "    # Step 2 of message-passing with DGL:\n",
    "    #   Reduce function collects all messages={hj, eij} sent to node dst/i with Step 1\n",
    "    #                   and sum/mean over the graph neigbors j in Ni\n",
    "    def reduce_func(self, nodes):\n",
    "        expij = nodes.mailbox['expij'] # size=(N,|Nj|,K,1), |Nj|=num_neighbors\n",
    "        vj = nodes.mailbox['vj'] # size=(N,|Nj|,K,d')\n",
    "        # Compute h_i = sum_j score_ij . v_j , where score_ij = exp_ij / sum_j' exp_ij'\n",
    "        #numerator = ### YOUR CODE HERE, sum_j exp_ij . v_j, size=(N,K,d')\n",
    "        #denominator = ### YOUR CODE HERE, sum_j' exp_ij', size=(N,K,1)\n",
    "        numerator = torch.sum( expij * vj, dim=1 ) # sum_j exp_ij . v_j, size=(N,K,d')\n",
    "        denominator = torch.sum( expij, dim=1 ) # sum_j' exp_ij', size=(N,K,1)\n",
    "        h = numerator / denominator # h_i = sum_j score_ij . v_j , where score_ij = exp_ij / sum_j' exp_ij', size=(N,K,d')\n",
    "        return {'h' : h}\n",
    "\n",
    "    def forward(self, g, h):\n",
    "        Q = self.WQ(h) # size=(N, d)\n",
    "                       # computational trick to compute quickly K linear transformations h_k.WQ of size=(N, d')\n",
    "                       # first compute linear transformation h.WQ of size=(N, d)\n",
    "                       # then reshape h.WQ of size=(N, K, d'=d/K)\n",
    "        K = self.WK(h) # size=(N, d)\n",
    "        V = self.WV(h) # size=(N, d)\n",
    "        g.ndata['Q'] = Q.view(-1, self.num_heads, self.head_hidden_dim) # size=(N, K, d'=d/K)\n",
    "        g.ndata['K'] = K.view(-1, self.num_heads, self.head_hidden_dim) # size=(N, K, d'=d/K)\n",
    "        g.ndata['V'] = V.view(-1, self.num_heads, self.head_hidden_dim) # size=(N, K, d'=d/K)\n",
    "        g.update_all(self.message_func, self.reduce_func) # compute with DGL the graph MHA\n",
    "        gMHA = g.ndata['h'] # size=(N, K, d'=d/K)\n",
    "        return gMHA\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "lWOIc6W_8JY4"
   },
   "outputs": [],
   "source": [
    "# class GraphTransformer layer\n",
    "class GraphTransformer_layer(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_dim, num_heads, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim # hidden_dim = d\n",
    "        self.num_heads = num_heads # number of heads = K\n",
    "        self.dropout_mha = nn.Dropout(dropout) # dropout value\n",
    "        self.dropout_mlp = nn.Dropout(dropout) # dropout value\n",
    "        self.gMHA = graph_MHA_layer(hidden_dim, hidden_dim//num_heads, num_heads) # graph MHA layer\n",
    "        self.WO = nn.Linear(hidden_dim, hidden_dim) # LL\n",
    "        self.layer_norm1 = nn.LayerNorm(hidden_dim) # layer normalization\n",
    "        self.layer_norm2 = nn.LayerNorm(hidden_dim)\n",
    "        self.linear1 = nn.Linear(hidden_dim, hidden_dim) # LL1 for MLP\n",
    "        self.linear2 = nn.Linear(hidden_dim, hidden_dim) # LL2 for MLP\n",
    "\n",
    "    def forward(self, g, h):\n",
    "\n",
    "        # Self-attention layer\n",
    "        h_rc = h # size=(N,d), V=num_nodes, for residual connection\n",
    "        h = self.layer_norm1(h) # layer normalization, size=(N, d)\n",
    "        h_MHA = self.gMHA(g, h) # MHA, size=(N, K, d'=d/K)\n",
    "        h_MHA = h_MHA.view(-1, self.hidden_dim) # size=(N, d)\n",
    "        h_MHA = self.dropout_mha(h_MHA) # dropout, size=(N, d)\n",
    "        h_MHA = self.WO(h_MHA) # LL, size=(N, d)\n",
    "        h = h_rc + h_MHA # residual connection, size=(N, d)\n",
    "\n",
    "        # Fully-connected layer\n",
    "        h_rc = h # for residual connection, size=(N, d)\n",
    "        h = self.layer_norm2(h) # layer normalization, size=(N, d)\n",
    "        h_MLP = self.linear1(h) # LL, size=(H, d)\n",
    "        h_MLP = torch.relu(h_MLP) # size=(N, d)\n",
    "        h_MLP = self.dropout_mlp(h_MLP) # dropout, size=(N, d)\n",
    "        h_MLP = self.linear2(h_MLP) # LL, size=(N, d)\n",
    "        h = h_rc + h_MLP # residual connection, size=(N, d)\n",
    "\n",
    "        return h\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "o_Gaf_Vi8MQt"
   },
   "outputs": [],
   "source": [
    "# class Graph Transformer network\n",
    "class GraphTransformer_net(nn.Module):\n",
    "\n",
    "    def __init__(self, net_parameters):\n",
    "        super(GraphTransformer_net, self).__init__()\n",
    "        input_dim = net_parameters['input_dim']\n",
    "        pos_enc_dim = net_parameters['pos_enc_dim']\n",
    "        hidden_dim = net_parameters['hidden_dim']\n",
    "        num_heads = net_parameters['num_heads']\n",
    "        L = net_parameters['L']\n",
    "        self.embedding_h = nn.Embedding(num_atom_type, hidden_dim)\n",
    "        self.embedding_pe = nn.Linear(pos_enc_dim, hidden_dim)\n",
    "        self.embedding_e = nn.Linear(1, hidden_dim)\n",
    "        self.GraphTransformer_layers = nn.ModuleList([ GraphTransformer_layer(hidden_dim, num_heads) for _ in range(L) ])\n",
    "        self.ln_h_final = nn.LayerNorm(hidden_dim)\n",
    "        self.linear_h_final = nn.Linear(hidden_dim, 1, bias=True)\n",
    "\n",
    "    def forward(self, g, h, pe):\n",
    "\n",
    "        # input node embedding = node in-degree feature\n",
    "        h = self.embedding_h(h) # in-degree feature, size=(num_nodes, hidden_dim)\n",
    "\n",
    "        # graph convnet layers\n",
    "        for GT_layer in self.GraphTransformer_layers:\n",
    "            h = GT_layer(g,h) # size=(num_nodes, hidden_dim)\n",
    "\n",
    "        # MLP classifier\n",
    "        g.ndata['h'] = h\n",
    "        mol_token = dgl.mean_nodes(g,'h') # DGL mean function over the neighbors, size=(num_graphs, hidden_dim)\n",
    "        y = self.ln_h_final(mol_token)\n",
    "        y = self.linear_h_final(y) # size=(num_graphs, 1)\n",
    "\n",
    "        return y\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1730637547939,
     "user": {
      "displayName": "Guoji Fu",
      "userId": "16398754709610840055"
     },
     "user_tz": -480
    },
    "id": "rPtk6EjKxXci",
    "outputId": "8db1c7ef-bbc3-4f67-df10-3ce74c4fac93"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 400641 (0.40 million)\n",
      "torch.Size([10, 1])\n"
     ]
    }
   ],
   "source": [
    "# Instantiate one network\n",
    "net_parameters = {}\n",
    "net_parameters['input_dim'] = 1\n",
    "net_parameters['pos_enc_dim'] = pos_enc_dim\n",
    "net_parameters['hidden_dim'] = 128\n",
    "net_parameters['num_heads'] = 8\n",
    "net_parameters['L'] = 4\n",
    "del net\n",
    "net = GraphTransformer_net(net_parameters)\n",
    "_ = display_num_param(net)\n",
    "\n",
    "batch_size = 10\n",
    "train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True, collate_fn=collate)\n",
    "batch_graphs, batch_labels = list(train_loader)[0]\n",
    "batch_x = batch_graphs.ndata['feat']\n",
    "batch_pe = batch_graphs.ndata['pos_enc']\n",
    "batch_labels = batch_labels\n",
    "batch_scores = net(batch_graphs, batch_x, batch_pe)\n",
    "print(batch_scores.size())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_jt8BzHJxXci"
   },
   "source": [
    "# Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 168398,
     "status": "ok",
     "timestamp": 1730637716334,
     "user": {
      "displayName": "Guoji Fu",
      "userId": "16398754709610840055"
     },
     "user_tz": -480
    },
    "id": "xAdlIhxXxXci",
    "outputId": "ec39dc6a-0677-4b9e-93a0-d79426d21d8e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 400641 (0.40 million)\n",
      "Epoch 0, time 1.9261, train_loss: 1.3590, test_loss: 1.3585\n",
      "Epoch 0, time 1.9261, train_loss: 1.3590, test_loss: 1.3585\n",
      "Epoch 1, time 3.7548, train_loss: 1.2445, test_loss: 1.2490\n",
      "Epoch 1, time 3.7548, train_loss: 1.2445, test_loss: 1.2490\n",
      "Epoch 2, time 5.6414, train_loss: 1.1653, test_loss: 1.1547\n",
      "Epoch 2, time 5.6414, train_loss: 1.1653, test_loss: 1.1547\n",
      "Epoch 3, time 7.4291, train_loss: 1.1438, test_loss: 1.1277\n",
      "Epoch 3, time 7.4291, train_loss: 1.1438, test_loss: 1.1277\n",
      "Epoch 4, time 9.2922, train_loss: 1.1030, test_loss: 1.1053\n",
      "Epoch 4, time 9.2922, train_loss: 1.1030, test_loss: 1.1053\n",
      "Epoch 5, time 11.1363, train_loss: 1.0779, test_loss: 1.0981\n",
      "Epoch 5, time 11.1363, train_loss: 1.0779, test_loss: 1.0981\n",
      "Epoch 6, time 12.9854, train_loss: 1.0766, test_loss: 1.1090\n",
      "Epoch 6, time 12.9854, train_loss: 1.0766, test_loss: 1.1090\n",
      "Epoch 7, time 14.8688, train_loss: 1.0471, test_loss: 1.1182\n",
      "Epoch 7, time 14.8688, train_loss: 1.0471, test_loss: 1.1182\n",
      "Epoch 8, time 16.6748, train_loss: 1.0731, test_loss: 1.0510\n",
      "Epoch 8, time 16.6748, train_loss: 1.0731, test_loss: 1.0510\n",
      "Epoch 9, time 18.5124, train_loss: 1.0207, test_loss: 0.9927\n",
      "Epoch 9, time 18.5124, train_loss: 1.0207, test_loss: 0.9927\n",
      "Epoch 10, time 20.3297, train_loss: 1.0137, test_loss: 1.0233\n",
      "Epoch 10, time 20.3297, train_loss: 1.0137, test_loss: 1.0233\n",
      "Epoch 11, time 22.2413, train_loss: 0.9867, test_loss: 0.9826\n",
      "Epoch 11, time 22.2413, train_loss: 0.9867, test_loss: 0.9826\n",
      "Epoch 12, time 24.2243, train_loss: 0.9644, test_loss: 0.9918\n",
      "Epoch 12, time 24.2243, train_loss: 0.9644, test_loss: 0.9918\n",
      "Epoch 13, time 26.1377, train_loss: 0.9989, test_loss: 1.0405\n",
      "Epoch 13, time 26.1377, train_loss: 0.9989, test_loss: 1.0405\n",
      "Epoch 14, time 28.0872, train_loss: 0.9688, test_loss: 0.9482\n",
      "Epoch 14, time 28.0872, train_loss: 0.9688, test_loss: 0.9482\n",
      "Epoch 15, time 30.0207, train_loss: 0.9690, test_loss: 0.9441\n",
      "Epoch 15, time 30.0207, train_loss: 0.9690, test_loss: 0.9441\n",
      "Epoch 16, time 31.8963, train_loss: 1.0252, test_loss: 0.9989\n",
      "Epoch 16, time 31.8963, train_loss: 1.0252, test_loss: 0.9989\n",
      "Epoch 17, time 33.8434, train_loss: 0.9793, test_loss: 0.9361\n",
      "Epoch 17, time 33.8434, train_loss: 0.9793, test_loss: 0.9361\n",
      "Epoch 18, time 36.3841, train_loss: 0.9504, test_loss: 0.9591\n",
      "Epoch 18, time 36.3841, train_loss: 0.9504, test_loss: 0.9591\n",
      "Epoch 19, time 38.9134, train_loss: 0.9488, test_loss: 1.0107\n",
      "Epoch 19, time 38.9134, train_loss: 0.9488, test_loss: 1.0107\n",
      "Epoch 20, time 41.0387, train_loss: 0.9371, test_loss: 0.9504\n",
      "Epoch 20, time 41.0387, train_loss: 0.9371, test_loss: 0.9504\n",
      "Epoch 21, time 42.9665, train_loss: 0.9321, test_loss: 0.9271\n",
      "Epoch 21, time 42.9665, train_loss: 0.9321, test_loss: 0.9271\n",
      "Epoch 22, time 45.3685, train_loss: 0.9228, test_loss: 1.0030\n",
      "Epoch 22, time 45.3685, train_loss: 0.9228, test_loss: 1.0030\n",
      "Epoch 23, time 47.6453, train_loss: 0.9370, test_loss: 0.9526\n",
      "Epoch 23, time 47.6453, train_loss: 0.9370, test_loss: 0.9526\n",
      "Epoch 24, time 50.4600, train_loss: 0.9091, test_loss: 0.9197\n",
      "Epoch 24, time 50.4600, train_loss: 0.9091, test_loss: 0.9197\n",
      "Epoch 25, time 52.8234, train_loss: 0.9042, test_loss: 0.9288\n",
      "Epoch 25, time 52.8234, train_loss: 0.9042, test_loss: 0.9288\n",
      "Epoch 26, time 54.7805, train_loss: 0.9016, test_loss: 0.9269\n",
      "Epoch 26, time 54.7805, train_loss: 0.9016, test_loss: 0.9269\n",
      "Epoch 27, time 56.8545, train_loss: 0.8928, test_loss: 0.9201\n",
      "Epoch 27, time 56.8545, train_loss: 0.8928, test_loss: 0.9201\n",
      "Epoch 28, time 59.1187, train_loss: 0.8931, test_loss: 0.9480\n",
      "Epoch 28, time 59.1187, train_loss: 0.8931, test_loss: 0.9480\n",
      "Epoch 29, time 61.3966, train_loss: 0.8995, test_loss: 0.9027\n",
      "Epoch 29, time 61.3966, train_loss: 0.8995, test_loss: 0.9027\n",
      "Epoch 30, time 63.4426, train_loss: 0.8783, test_loss: 0.9035\n",
      "Epoch 30, time 63.4426, train_loss: 0.8783, test_loss: 0.9035\n",
      "Epoch 31, time 65.6110, train_loss: 0.8791, test_loss: 0.8996\n",
      "Epoch 31, time 65.6110, train_loss: 0.8791, test_loss: 0.8996\n",
      "Epoch 32, time 67.6798, train_loss: 0.8856, test_loss: 0.9467\n",
      "Epoch 32, time 67.6798, train_loss: 0.8856, test_loss: 0.9467\n",
      "Epoch 33, time 69.6616, train_loss: 0.8843, test_loss: 0.9502\n",
      "Epoch 33, time 69.6616, train_loss: 0.8843, test_loss: 0.9502\n",
      "Epoch 34, time 71.6070, train_loss: 0.8796, test_loss: 0.9040\n",
      "Epoch 34, time 71.6070, train_loss: 0.8796, test_loss: 0.9040\n",
      "Epoch 35, time 73.5282, train_loss: 0.8692, test_loss: 0.9167\n",
      "Epoch 35, time 73.5282, train_loss: 0.8692, test_loss: 0.9167\n",
      "Epoch 36, time 75.4071, train_loss: 0.8448, test_loss: 0.9235\n",
      "Epoch 36, time 75.4071, train_loss: 0.8448, test_loss: 0.9235\n",
      "Epoch 37, time 77.2384, train_loss: 0.8687, test_loss: 0.9367\n",
      "Epoch 37, time 77.2384, train_loss: 0.8687, test_loss: 0.9367\n",
      "Epoch 38, time 79.0072, train_loss: 0.8628, test_loss: 0.9897\n",
      "Epoch 38, time 79.0072, train_loss: 0.8628, test_loss: 0.9897\n",
      "Epoch 39, time 81.2952, train_loss: 0.8611, test_loss: 0.9253\n",
      "Epoch 39, time 81.2952, train_loss: 0.8611, test_loss: 0.9253\n",
      "Epoch 40, time 83.5410, train_loss: 0.8528, test_loss: 0.9391\n",
      "Epoch 40, time 83.5410, train_loss: 0.8528, test_loss: 0.9391\n",
      "Epoch 41, time 85.5073, train_loss: 0.8640, test_loss: 0.8967\n",
      "Epoch 41, time 85.5073, train_loss: 0.8640, test_loss: 0.8967\n",
      "Epoch 42, time 88.0346, train_loss: 0.8403, test_loss: 0.8915\n",
      "Epoch 42, time 88.0346, train_loss: 0.8403, test_loss: 0.8915\n",
      "Epoch 43, time 89.9912, train_loss: 0.8619, test_loss: 0.9275\n",
      "Epoch 43, time 89.9912, train_loss: 0.8619, test_loss: 0.9275\n",
      "Epoch 44, time 91.9417, train_loss: 0.8364, test_loss: 0.9074\n",
      "Epoch 44, time 91.9417, train_loss: 0.8364, test_loss: 0.9074\n",
      "Epoch 45, time 93.9642, train_loss: 0.8343, test_loss: 0.8964\n",
      "Epoch 45, time 93.9642, train_loss: 0.8343, test_loss: 0.8964\n",
      "Epoch 46, time 96.3067, train_loss: 0.8378, test_loss: 0.8930\n",
      "Epoch 46, time 96.3067, train_loss: 0.8378, test_loss: 0.8930\n",
      "Epoch 47, time 98.4193, train_loss: 0.8398, test_loss: 0.9097\n",
      "Epoch 47, time 98.4193, train_loss: 0.8398, test_loss: 0.9097\n",
      "Epoch 48, time 100.2919, train_loss: 0.8175, test_loss: 0.9399\n",
      "Epoch 48, time 100.2919, train_loss: 0.8175, test_loss: 0.9399\n",
      "Epoch 49, time 102.0917, train_loss: 0.8587, test_loss: 0.9427\n",
      "Epoch 49, time 102.0917, train_loss: 0.8587, test_loss: 0.9427\n",
      "Epoch 50, time 104.6672, train_loss: 0.8296, test_loss: 0.9033\n",
      "Epoch 50, time 104.6672, train_loss: 0.8296, test_loss: 0.9033\n",
      "Epoch 51, time 106.7326, train_loss: 0.8237, test_loss: 0.9258\n",
      "Epoch 51, time 106.7326, train_loss: 0.8237, test_loss: 0.9258\n",
      "Epoch 52, time 108.7941, train_loss: 0.8415, test_loss: 0.8878\n",
      "Epoch 52, time 108.7941, train_loss: 0.8415, test_loss: 0.8878\n",
      "Epoch 53, time 110.7855, train_loss: 0.8200, test_loss: 0.9255\n",
      "Epoch 53, time 110.7855, train_loss: 0.8200, test_loss: 0.9255\n",
      "Epoch 54, time 112.8333, train_loss: 0.8379, test_loss: 0.9051\n",
      "Epoch 54, time 112.8333, train_loss: 0.8379, test_loss: 0.9051\n",
      "Epoch 55, time 114.8087, train_loss: 0.8236, test_loss: 0.9450\n",
      "Epoch 55, time 114.8087, train_loss: 0.8236, test_loss: 0.9450\n",
      "Epoch 56, time 116.7087, train_loss: 0.8017, test_loss: 0.8961\n",
      "Epoch 56, time 116.7087, train_loss: 0.8017, test_loss: 0.8961\n",
      "Epoch 57, time 118.6929, train_loss: 0.8138, test_loss: 0.9152\n",
      "Epoch 57, time 118.6929, train_loss: 0.8138, test_loss: 0.9152\n",
      "Epoch 58, time 121.2532, train_loss: 0.7835, test_loss: 0.9049\n",
      "Epoch 58, time 121.2532, train_loss: 0.7835, test_loss: 0.9049\n",
      "Epoch 59, time 124.0260, train_loss: 0.8048, test_loss: 0.8891\n",
      "Epoch 59, time 124.0260, train_loss: 0.8048, test_loss: 0.8891\n",
      "Epoch 60, time 126.1955, train_loss: 0.8134, test_loss: 0.9189\n",
      "Epoch 60, time 126.1955, train_loss: 0.8134, test_loss: 0.9189\n",
      "Epoch 61, time 128.2198, train_loss: 0.8020, test_loss: 0.9073\n",
      "Epoch 61, time 128.2198, train_loss: 0.8020, test_loss: 0.9073\n",
      "Epoch 62, time 130.4331, train_loss: 0.7730, test_loss: 0.8981\n",
      "Epoch 62, time 130.4331, train_loss: 0.7730, test_loss: 0.8981\n",
      "Epoch 63, time 132.3566, train_loss: 0.7769, test_loss: 0.9077\n",
      "Epoch 63, time 132.3566, train_loss: 0.7769, test_loss: 0.9077\n",
      "Epoch 64, time 134.4575, train_loss: 0.7555, test_loss: 0.9063\n",
      "Epoch 64, time 134.4575, train_loss: 0.7555, test_loss: 0.9063\n",
      "Epoch 65, time 136.3505, train_loss: 0.7626, test_loss: 0.9353\n",
      "Epoch 65, time 136.3505, train_loss: 0.7626, test_loss: 0.9353\n",
      "Epoch 66, time 138.2708, train_loss: 0.7785, test_loss: 0.9018\n",
      "Epoch 66, time 138.2708, train_loss: 0.7785, test_loss: 0.9018\n",
      "Epoch 67, time 140.2566, train_loss: 0.7561, test_loss: 0.9060\n",
      "Epoch 67, time 140.2566, train_loss: 0.7561, test_loss: 0.9060\n",
      "Epoch 68, time 142.1290, train_loss: 0.7485, test_loss: 0.9183\n",
      "Epoch 68, time 142.1290, train_loss: 0.7485, test_loss: 0.9183\n",
      "Epoch 69, time 144.1180, train_loss: 0.7514, test_loss: 0.9573\n",
      "Epoch 69, time 144.1180, train_loss: 0.7514, test_loss: 0.9573\n",
      "Epoch 70, time 146.0106, train_loss: 0.7448, test_loss: 0.9131\n",
      "Epoch 70, time 146.0106, train_loss: 0.7448, test_loss: 0.9131\n",
      "Epoch 71, time 148.0541, train_loss: 0.7400, test_loss: 0.9272\n",
      "Epoch 71, time 148.0541, train_loss: 0.7400, test_loss: 0.9272\n",
      "Epoch 72, time 149.9999, train_loss: 0.7467, test_loss: 0.9449\n",
      "Epoch 72, time 149.9999, train_loss: 0.7467, test_loss: 0.9449\n",
      "Epoch 73, time 151.9757, train_loss: 0.7516, test_loss: 0.9215\n",
      "Epoch 73, time 151.9757, train_loss: 0.7516, test_loss: 0.9215\n",
      "Epoch 74, time 154.0749, train_loss: 0.7299, test_loss: 0.9228\n",
      "Epoch 74, time 154.0749, train_loss: 0.7299, test_loss: 0.9228\n",
      "Epoch 75, time 156.0777, train_loss: 0.7133, test_loss: 0.9400\n",
      "Epoch 75, time 156.0777, train_loss: 0.7133, test_loss: 0.9400\n",
      "Epoch 76, time 158.9299, train_loss: 0.7438, test_loss: 0.9074\n",
      "Epoch 76, time 158.9299, train_loss: 0.7438, test_loss: 0.9074\n",
      "Epoch 77, time 161.7469, train_loss: 0.7184, test_loss: 0.9335\n",
      "Epoch 77, time 161.7469, train_loss: 0.7184, test_loss: 0.9335\n",
      "Epoch 78, time 164.9020, train_loss: 0.7468, test_loss: 0.9428\n",
      "Epoch 78, time 164.9020, train_loss: 0.7468, test_loss: 0.9428\n",
      "Epoch 79, time 167.9541, train_loss: 0.7560, test_loss: 0.9155\n",
      "Epoch 79, time 167.9541, train_loss: 0.7560, test_loss: 0.9155\n",
      "Epoch 80, time 171.0821, train_loss: 0.7283, test_loss: 0.9304\n",
      "Epoch 80, time 171.0821, train_loss: 0.7283, test_loss: 0.9304\n",
      "Epoch 81, time 173.9502, train_loss: 0.6895, test_loss: 0.9579\n",
      "Epoch 81, time 173.9502, train_loss: 0.6895, test_loss: 0.9579\n",
      "Epoch 82, time 177.1516, train_loss: 0.6886, test_loss: 0.9579\n",
      "Epoch 82, time 177.1516, train_loss: 0.6886, test_loss: 0.9579\n",
      "Epoch 83, time 179.9958, train_loss: 0.7167, test_loss: 0.9173\n",
      "Epoch 83, time 179.9958, train_loss: 0.7167, test_loss: 0.9173\n",
      "Epoch 84, time 182.9659, train_loss: 0.7068, test_loss: 0.9451\n",
      "Epoch 84, time 182.9659, train_loss: 0.7068, test_loss: 0.9451\n",
      "Epoch 85, time 185.9248, train_loss: 0.7091, test_loss: 0.9353\n",
      "Epoch 85, time 185.9248, train_loss: 0.7091, test_loss: 0.9353\n",
      "Epoch 86, time 188.7009, train_loss: 0.7065, test_loss: 0.9443\n",
      "Epoch 86, time 188.7009, train_loss: 0.7065, test_loss: 0.9443\n",
      "Epoch 87, time 191.5150, train_loss: 0.6726, test_loss: 0.9724\n",
      "Epoch 87, time 191.5150, train_loss: 0.6726, test_loss: 0.9724\n",
      "Epoch 88, time 194.2880, train_loss: 0.6819, test_loss: 0.9087\n",
      "Epoch 88, time 194.2880, train_loss: 0.6819, test_loss: 0.9087\n",
      "Epoch 89, time 197.3105, train_loss: 0.6640, test_loss: 0.9631\n",
      "Epoch 89, time 197.3105, train_loss: 0.6640, test_loss: 0.9631\n",
      "Epoch 90, time 200.4857, train_loss: 0.6887, test_loss: 0.9780\n",
      "Epoch 90, time 200.4857, train_loss: 0.6887, test_loss: 0.9780\n",
      "Epoch 91, time 203.4424, train_loss: 0.6917, test_loss: 0.9666\n",
      "Epoch 91, time 203.4424, train_loss: 0.6917, test_loss: 0.9666\n",
      "Epoch 92, time 206.4356, train_loss: 0.6668, test_loss: 0.9472\n",
      "Epoch 92, time 206.4356, train_loss: 0.6668, test_loss: 0.9472\n",
      "Epoch 93, time 209.2647, train_loss: 0.6696, test_loss: 0.9631\n",
      "Epoch 93, time 209.2647, train_loss: 0.6696, test_loss: 0.9631\n",
      "Epoch 94, time 211.9989, train_loss: 0.6652, test_loss: 0.9418\n",
      "Epoch 94, time 211.9989, train_loss: 0.6652, test_loss: 0.9418\n",
      "Epoch 95, time 214.9590, train_loss: 0.6562, test_loss: 0.9705\n",
      "Epoch 95, time 214.9590, train_loss: 0.6562, test_loss: 0.9705\n",
      "Epoch 96, time 217.7540, train_loss: 0.6626, test_loss: 0.9827\n",
      "Epoch 96, time 217.7540, train_loss: 0.6626, test_loss: 0.9827\n",
      "Epoch 97, time 220.6160, train_loss: 0.6416, test_loss: 0.9443\n",
      "Epoch 97, time 220.6160, train_loss: 0.6416, test_loss: 0.9443\n",
      "Epoch 98, time 223.6326, train_loss: 0.6612, test_loss: 0.9708\n",
      "Epoch 98, time 223.6326, train_loss: 0.6612, test_loss: 0.9708\n",
      "Epoch 99, time 226.5981, train_loss: 0.6804, test_loss: 0.9795\n",
      "Epoch 99, time 226.5981, train_loss: 0.6804, test_loss: 0.9795\n"
     ]
    }
   ],
   "source": [
    "def run_one_epoch(net, data_loader, train=True, loss_fc=None, optimizer=None):\n",
    "    if train:\n",
    "        net.train() # during training\n",
    "    else:\n",
    "        net.eval()  # during inference/test\n",
    "    epoch_loss = 0\n",
    "    nb_data = 0\n",
    "    for iter, (batch_graphs, batch_labels) in enumerate(data_loader):\n",
    "        batch_x = batch_graphs.ndata['feat']\n",
    "        bs2 = batch_labels.size(0)\n",
    "        batch_pe = batch_graphs.ndata['pos_enc']\n",
    "        batch_pe = batch_pe * ( 2 * torch.randint(low=0, high=2, size=(1,pos_enc_dim)).float() - 1.0 ) # randomly flip sign of eigenvectors\n",
    "        batch_labels = batch_labels\n",
    "        batch_scores = net.forward(batch_graphs, batch_x, batch_pe)\n",
    "        lossMAE = loss_fc(batch_scores, batch_labels)\n",
    "        if train: # during training, run backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            lossMAE.backward()\n",
    "            optimizer.step()\n",
    "        epoch_loss += bs2 * lossMAE.detach().item()\n",
    "        nb_data += bs2\n",
    "    epoch_loss /= nb_data\n",
    "    return epoch_loss, optimizer\n",
    "\n",
    "\n",
    "# dataset loaders\n",
    "batch_size = 50\n",
    "train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True, collate_fn=collate)\n",
    "test_loader = DataLoader(testset, batch_size=batch_size, shuffle=False, collate_fn=collate)\n",
    "val_loader = DataLoader(valset, batch_size=batch_size, shuffle=False, drop_last=False, collate_fn=collate)\n",
    "\n",
    "# Instantiate one network\n",
    "net_parameters = {}\n",
    "net_parameters['input_dim'] = 1\n",
    "net_parameters['pos_enc_dim'] = pos_enc_dim\n",
    "net_parameters['hidden_dim'] = 128\n",
    "net_parameters['num_heads'] = 8\n",
    "net_parameters['L'] = 4\n",
    "del net\n",
    "net = GraphTransformer_net(net_parameters)\n",
    "def display_num_param(net):\n",
    "    nb_param = 0\n",
    "    for param in net.parameters():\n",
    "        nb_param += param.numel()\n",
    "    print('Number of parameters: {} ({:.2f} million)'.format(nb_param, nb_param/1e6))\n",
    "    return nb_param/1e6\n",
    "_ = display_num_param(net)\n",
    "\n",
    "# loss, optimizer\n",
    "lossMAE = nn.L1Loss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.0003)\n",
    "# optimizer = torch.optim.Adam(net.parameters(), lr=0.0001)\n",
    "\n",
    "# training loop\n",
    "start = time.time()\n",
    "for epoch in range(100):\n",
    "    epoch_train_loss, optimizer = run_one_epoch(net, train_loader, True, lossMAE, optimizer)\n",
    "    with torch.no_grad():\n",
    "        epoch_test_loss = run_one_epoch(net, test_loader, False, lossMAE)[0]\n",
    "        # epoch_val_loss = run_one_epoch(net, val_loader, False, lossMAE)[0]\n",
    "    print('Epoch {}, time {:.4f}, train_loss: {:.4f}, test_loss: {:.4f}'.format(epoch, time.time()-start, epoch_train_loss, epoch_test_loss))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n8uBSgqTxXcj"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LWLI02VPxXcj"
   },
   "source": [
    "## Compare results\n",
    "\n",
    "| GNN    | train MAE | test MAE |\n",
    "| -------- | ------- | ------- |\n",
    "| GT w/ edge features (bond type)    | 0.4762 | 0.7021     |\n",
    "| GT without edge features (only atom type)    | 0.6804    | 0.9795     |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d26mlnkvxXcj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j73ydTFQxXcj"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "gnn_course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
